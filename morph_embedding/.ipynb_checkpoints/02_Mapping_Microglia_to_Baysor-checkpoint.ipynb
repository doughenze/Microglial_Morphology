{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ba42d6e9-70b0-4c2d-944e-ed7de11936cf",
   "metadata": {},
   "source": [
    "# This notebook must be run with the Vizgen_2 conda environment within the Vizgen.sif singularity container"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81d49125-42a9-402c-b994-45f6511be179",
   "metadata": {},
   "outputs": [],
   "source": [
    "import Mapping\n",
    "import os\n",
    "import string\n",
    "\n",
    "import cv2\n",
    "import geopandas as gpd\n",
    "import igraph as ig\n",
    "import leidenalg\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import rasterio\n",
    "import seaborn as sns\n",
    "import tifffile\n",
    "import umap\n",
    "from anndata import AnnData as ad\n",
    "from matplotlib import patches as mpatches\n",
    "from matplotlib_scalebar.scalebar import ScaleBar\n",
    "import scanpy as sc\n",
    "import anndata\n",
    "from shapely.affinity import translate\n",
    "from shapely.geometry import Polygon, MultiPolygon, box, shape\n",
    "from skimage import img_as_bool, img_as_ubyte\n",
    "from skimage.measure import label, find_contours, regionprops, regionprops_table\n",
    "from skimage.morphology import skeletonize\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.neighbors import kneighbors_graph\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tqdm import tqdm\n",
    "import geojson\n",
    "from adjustText import adjust_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30887253-31cd-448f-ae29-a509bbc6e99b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_parquet_file(experiment_path):\n",
    "    region_types = ['region_0', 'region_1']\n",
    "    for region in region_types:\n",
    "        file_path = f'{experiment_path}baysor/{region}_6-5_micron_polygons.parquet'\n",
    "        if os.path.exists(file_path):\n",
    "            return file_path\n",
    "    return None\n",
    "\n",
    "def clean_df(dataframe, sc_obj):\n",
    "    \n",
    "    rows_to_keep = sc_obj.obs.Name.unique().tolist()\n",
    "    \n",
    "    new_df = dataframe[dataframe.Name.isin(rows_to_keep)]\n",
    "    new_df['Label_pixels'] = new_df['Label_pixels'].apply(lambda x: [i for i in x if i != 0])\n",
    "    new_df = new_df.drop_duplicates(subset='Name', keep='first')\n",
    "    df_single_label = new_df[new_df['Label_pixels'].apply(lambda x: len(x) == 1)]\n",
    "    single_label_values = df_single_label['Label_pixels'].apply(lambda x: x[0])\n",
    "    label_counts = single_label_values.value_counts()\n",
    "    unique_labels = label_counts[label_counts == 1].index\n",
    "    df_filtered = df_single_label[single_label_values.isin(unique_labels)]\n",
    "    df_filtered['Label_pixels'] = df_filtered['Label_pixels'].apply(lambda x: x[0])\n",
    "    \n",
    "    return df_filtered\n",
    "\n",
    "def subset_label_image(label_image, filt_df):\n",
    "    label_pixels_to_keep = filt_df['Label_pixels'].tolist()\n",
    "    mask = np.isin(test_im, label_pixels_to_keep)\n",
    "\n",
    "    # Subset the label image\n",
    "    subset_label_image = np.where(mask, label_image, 0)\n",
    "    return subset_label_image\n",
    "\n",
    "def morph_reduction(morph_df):\n",
    "    # dimensional analysis on the morphology stuff\n",
    "    # Select columns to exclude from PCA\n",
    "    columns_to_exclude = ['cell_label','local_centroid-0','local_centroid-1']\n",
    "\n",
    "    # Select only the numeric columns excluding specified columns\n",
    "    data_for_pca = features_mic.select_dtypes(include=[np.number]).drop(columns=columns_to_exclude, errors='ignore')\n",
    "\n",
    "    # Standardize the data\n",
    "    scaler = StandardScaler()\n",
    "    data_scaled = scaler.fit_transform(data_for_pca)\n",
    "    \n",
    "    pca = PCA(n_components=None)  # None will choose the minimum number of components that explain all the variance\n",
    "\n",
    "    # Fit and transform the scaled data\n",
    "    principal_components = pca.fit_transform(data_scaled)\n",
    "\n",
    "    # Create a DataFrame with the principal components\n",
    "    pca_df = pd.DataFrame(data=principal_components, columns=['PC' + str(i+1) for i in range(principal_components.shape[1])])\n",
    "\n",
    "    # Join the PCA DataFrame back with the excluded columns for full analysis\n",
    "    return pd.concat([features_mic['cell_label'].reset_index(drop=True), pca_df], axis=1)\n",
    "\n",
    "def morph_umap(pca_df):\n",
    "    pca_components = pca_df[['PC1', 'PC2', 'PC3','PC4', 'PC5','PC6']].values\n",
    "    \n",
    "    reducer = umap.UMAP(n_components=2, random_state=42)\n",
    "    umap_results = reducer.fit_transform(pca_components)\n",
    "    pca_df['geoUMAP-1'] = umap_results[:, 0]\n",
    "    pca_df['geoUMAP-2'] = umap_results[:, 1]\n",
    "    \n",
    "    return pca_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79f09533-a34c-4941-ad45-2542e290e6b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to skeletonize segmented cells\n",
    "def skeletonize_cells(labeled_cells):\n",
    "    # Convert labels to boolean\n",
    "    binary_cells = labeled_cells > 0\n",
    "    # Skeletonize the binary image\n",
    "    skeleton = skeletonize(binary_cells)\n",
    "    return skeleton\n",
    "\n",
    "# Function to analyze skeleton features\n",
    "def analyze_skeleton(skeleton):\n",
    "    # Compute the medial axis (skeleton with distance transform)\n",
    "    skel, distance = medial_axis(skeleton, return_distance=True)\n",
    "    # Distance to the background\n",
    "    dist_on_skel = distance * skel\n",
    "    return skel, dist_on_skel\n",
    "\n",
    "def skeleton_to_graph(skeleton):\n",
    "    graph = nx.Graph()\n",
    "    rows, cols = skeleton.shape\n",
    "    for index, value in np.ndenumerate(skeleton):\n",
    "        if value:\n",
    "            x, y = index\n",
    "            for dx, dy in [(0, 1), (1, 0), (1, 1), (1, -1)]:\n",
    "                neighbor_x, neighbor_y = x + dx, y + dy\n",
    "                if 0 <= neighbor_x < rows and 0 <= neighbor_y < cols:\n",
    "                    neighbor = (neighbor_x, neighbor_y)\n",
    "                    if skeleton[neighbor]:\n",
    "                        if not graph.has_edge(index, neighbor):\n",
    "                            weight = np.linalg.norm(np.array(index) - np.array(neighbor))\n",
    "                            graph.add_edge(index, neighbor, weight=weight)\n",
    "    return graph\n",
    "\n",
    "def extract_sub_image_with_padding(image, bbox, padding=10):\n",
    "    min_row, min_col, max_row, max_col = bbox\n",
    "    min_row = max(min_row - padding, 0)\n",
    "    min_col = max(min_col - padding, 0)\n",
    "    max_row = min(max_row + padding, image.shape[0])\n",
    "    max_col = min(max_col + padding, image.shape[1])\n",
    "    return image[min_row:max_row, min_col:max_col], (min_row, min_col)\n",
    "\n",
    "def extract_features_to_dataframe(labeled_cells):\n",
    "    data = []\n",
    "    regions = regionprops(labeled_cells)\n",
    "    \n",
    "    # Initialize the progress bar\n",
    "    for region in tqdm(regions, desc=\"Processing cells\"):\n",
    "        isolated_cell, offset = extract_sub_image_with_padding(labeled_cells, region.bbox)\n",
    "        isolated_cell = isolated_cell == region.label\n",
    "        isolated_cell = isolated_cell.astype(int)  # ensure it's an integer label image\n",
    "        \n",
    "        skeleton = skeletonize(isolated_cell)\n",
    "        \n",
    "        graph = skeleton_to_graph(skeleton)\n",
    "        \n",
    "        branch_points = list(nx.articulation_points(graph))\n",
    "        endpoints = [x for x in graph.nodes() if graph.degree(x) == 1]\n",
    "        total_length = np.sum(skeleton)\n",
    "        num_branches = len(branch_points)\n",
    "        branch_lengths = [graph[u][v]['weight'] for u, v in graph.edges()]\n",
    "        tortuosity = [graph[u][v]['weight'] / np.linalg.norm(np.array(u) - np.array(v)) for u, v in graph.edges()]\n",
    "\n",
    "        # Extract geometric properties\n",
    "        properties = regionprops_table(isolated_cell, properties=[\n",
    "            'area', 'convex_area', 'eccentricity', 'equivalent_diameter',\n",
    "            'extent', 'filled_area', 'major_axis_length', 'minor_axis_length',\n",
    "            'orientation', 'perimeter', 'solidity', 'bbox_area', 'feret_diameter_max',\n",
    "            'local_centroid', 'moments_hu', 'euler_number'\n",
    "        ])\n",
    "        properties_df = pd.DataFrame(properties)\n",
    "\n",
    "        # Combine data into one dictionary per region\n",
    "        cell_data = {\n",
    "            'cell_label': region.label,\n",
    "            'total_length': total_length,\n",
    "            'number_of_branches': num_branches,\n",
    "            'branch_lengths': branch_lengths,\n",
    "            'num_endpoints': len(endpoints),\n",
    "            'branch_points': len(branch_points),\n",
    "            'tortuosity': tortuosity\n",
    "        }\n",
    "\n",
    "        # Add region props features\n",
    "        for key, value in properties_df.iloc[0].items():\n",
    "            cell_data[key] = value\n",
    "\n",
    "        data.append(cell_data)\n",
    "\n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "def calculate_ramification_features(df):\n",
    "    # Calculate ramification metrics and add them to the DataFrame\n",
    "    df['branch_point_ratio'] = df['branch_points'] / df['total_length']\n",
    "    df['endpoint_to_branch_point_ratio'] = df['num_endpoints'] / df['branch_points']\n",
    "    df['average_branch_length'] = df['total_length'] / df['number_of_branches']\n",
    "    df['ramification_index'] = (df['perimeter']/df['area'])/(2*np.sqrt(np.pi/df['area']))\n",
    "\n",
    "    # Handle cases where there are no branches to avoid division by zero\n",
    "    df['endpoint_to_branch_point_ratio'] = df['endpoint_to_branch_point_ratio'].fillna(0)\n",
    "    df['average_branch_length'] = df['average_branch_length'].fillna(0)\n",
    "\n",
    "    return df\n",
    "\n",
    "def scale_measurements(df, micron_per_pixel):\n",
    "    # Define which columns to scale and by what factor\n",
    "    pixel_measurements = ['perimeter', 'equivalent_diameter', 'major_axis_length', 'minor_axis_length']\n",
    "    area_measurements = ['area', 'convex_area']\n",
    "\n",
    "    # Scale measurements that are linear dimensions\n",
    "    for column in pixel_measurements:\n",
    "        df[column] *= micron_per_pixel\n",
    "\n",
    "    # Scale measurements that are area dimensions\n",
    "    for column in area_measurements:\n",
    "        df[column] *= (micron_per_pixel ** 2)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a730090-c945-43aa-b1a4-a18cf012c67a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ad_parent = sc.read_h5ad('../Baysor/ABC_cleaned.h5ad')\n",
    "ad_parent = ad_parent[ad_parent.obs.subclass_label_transfer == 'Microglia NN']\n",
    "\n",
    "# accept the base path\n",
    "experiment_base_paths = ['/hpc/projects/group.quake/doug/Shapes_Spatial/3-mo-male-1/',\n",
    "                         '/hpc/projects/group.quake/doug/Shapes_Spatial/3-mo-male-2/',\n",
    "                         '/hpc/projects/group.quake/doug/Shapes_Spatial/3-mo-male-3-rev2/',\n",
    "                         '/hpc/projects/group.quake/doug/Shapes_Spatial/3-mo-female-1-rev2/',\n",
    "                         '/hpc/projects/group.quake/doug/Shapes_Spatial/3-mo-female-2/',\n",
    "                         '/hpc/projects/group.quake/doug/Shapes_Spatial/3-mo-female-3/',\n",
    "                         '/hpc/projects/group.quake/doug/Shapes_Spatial/24-mo-male-1/',\n",
    "                         '/hpc/projects/group.quake/doug/Shapes_Spatial/24-mo-male-2/',\n",
    "                         '/hpc/projects/group.quake/doug/Shapes_Spatial/24-mo-male-4-rev2/',\n",
    "                         '/hpc/projects/group.quake/doug/Shapes_Spatial/24-mo-female-1/',\n",
    "                         '/hpc/projects/group.quake/doug/Shapes_Spatial/24-mo-female-3/',\n",
    "                         '/hpc/projects/group.quake/doug/Shapes_Spatial/24-mo-female-5/']\n",
    "\n",
    "ad_bulk = []\n",
    "\n",
    "for experiment in experiment_base_paths:\n",
    "    batch = experiment.split('/')[-2]\n",
    "    \n",
    "    # Okay now we load our image from this base path\n",
    "    test_im = Mapping.load_tiff_image(experiment + 'labeled_image.tif')\n",
    "    print(f\"Labeled Image Loaded for {batch}\")\n",
    "    # We also want to load in the geometry file\n",
    "    parquet_file = find_parquet_file(experiment)\n",
    "    boundaries = gpd.read_parquet(parquet_file)\n",
    "    # then we want to take a subset from our adata which corresponds to the file path\n",
    "    ad_viz = ad_parent[ad_parent.obs.batchID == batch]\n",
    "    \n",
    "    data = Mapping.extract_label_pixel_values_baysor(boundaries, test_im)    \n",
    "    filtered_df = clean_df(pd.DataFrame(data),ad_viz)\n",
    "    \n",
    "    sub_im = subset_label_image(test_im, filtered_df)\n",
    "    \n",
    "    features_df = extract_features_to_dataframe(sub_im)\n",
    "    \n",
    "    features_mic = calculate_ramification_features(features_df)\n",
    "\n",
    "    #final_pca_df = morph_reduction(features_mic)\n",
    "    \n",
    "    #final_pca_df = morph_umap(final_pca_df)\n",
    "    \n",
    "    merged_df = pd.merge(filtered_df, features_mic, left_on='Label_pixels', right_on='cell_label', how='inner')\n",
    "    merged_df = merged_df.drop(columns=['branch_lengths', 'tortuosity', 'local_centroid-0', 'local_centroid-1'])\n",
    "    \n",
    "    rows_to_keep = merged_df.Name.unique().tolist()\n",
    "    \n",
    "    test_ad = ad_viz[ad_viz.obs.Name.isin(rows_to_keep)]\n",
    "    \n",
    "    test_ad.obs = pd.merge(test_ad.obs,merged_df,left_on='Name', right_on='Name', how='inner')\n",
    "    \n",
    "    ad_bulk.append(test_ad)\n",
    "concatenated_data = sc.concat(ad_bulk, join='outer')\n",
    "concatenated_data.obs_names_make_unique()\n",
    "concatenated_data.obs = concatenated_data.obs.drop('Percentage', axis=1)\n",
    "concatenated_data.write_h5ad('Microglia_Shapespace_baysor_500.h5ad')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Vizgen_2",
   "language": "python",
   "name": "vizgen_2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
