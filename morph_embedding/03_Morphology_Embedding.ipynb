{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "52e896d1-ac48-4905-9ceb-e6b4be6f26dc",
   "metadata": {},
   "source": [
    "This code is largely inspired by the repo which can be seen here: https://gitlab.mpcdf.mpg.de/mpibr/laur/cuttlefish/cuttlefish-code-public"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17c20168-6194-40af-9d77-d7e331edfadd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import Mapping\n",
    "import os\n",
    "import string\n",
    "\n",
    "import cv2\n",
    "import geopandas as gpd\n",
    "import igraph as ig\n",
    "#import leidenalg\n",
    "import matplotlib.pyplot as plt\n",
    "#import networkx as nx\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "#import rasterio\n",
    "import seaborn as sns\n",
    "import tifffile\n",
    "#import umap\n",
    "from anndata import AnnData as ad\n",
    "from matplotlib import patches as mpatches\n",
    "#from matplotlib_scalebar.scalebar import ScaleBar\n",
    "import scanpy as sc\n",
    "from shapely.affinity import translate\n",
    "from shapely.geometry import Polygon, MultiPolygon, box, shape\n",
    "from skimage import img_as_bool, img_as_ubyte\n",
    "from skimage.measure import label, find_contours, regionprops, regionprops_table\n",
    "from skimage.morphology import skeletonize\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.neighbors import kneighbors_graph\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tqdm import tqdm\n",
    "import geojson\n",
    "import json\n",
    "\n",
    "from tensorflow.keras.applications import vgg19\n",
    "from keract import get_activations\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4d5549e-a009-42bd-99b0-584bbaf6c28b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_sub_image_with_padding(image, bbox, padding=10):\n",
    "    min_row, min_col, max_row, max_col = bbox\n",
    "    min_row = max(min_row - padding, 0)\n",
    "    min_col = max(min_col - padding, 0)\n",
    "    max_row = min(max_row + padding, image.shape[0])\n",
    "    max_col = min(max_col + padding, image.shape[1])\n",
    "    return image[min_row:max_row, min_col:max_col], (min_row, min_col)\n",
    "\n",
    "def extract_sub_image_with_padding_and_threshold(image, bbox, padding=0):\n",
    "    min_row, min_col, max_row, max_col = bbox\n",
    "    padded_image = np.pad(image, ((padding, padding), (padding, padding)), mode='constant')\n",
    "    sub_image = padded_image[min_row:max_row + 2*padding, min_col:max_col + 2*padding]\n",
    "    sub_image[sub_image > 0] = 255\n",
    "    return sub_image\n",
    "\n",
    "def load_label_image(test_im, raw_im):\n",
    "    regions = regionprops(test_im)\n",
    "    test_label = {}\n",
    "    for region in regions:\n",
    "            # change this around if it is broken\n",
    "            test_label[region.label] = extract_sub_image_with_padding(raw_im, region.bbox)\n",
    "            #test_label[region.label] = extract_sub_image_with_padding_and_threshold(test_im.astype(np.uint8), region.bbox, padding=10)\n",
    "\n",
    "    return test_label\n",
    "\n",
    "def vectorize_imaging_data(data, batch_list):\n",
    "    root = '/hpc/projects/group.quake/doug/Shapes_Spatial/'\n",
    "    total = []\n",
    "    \n",
    "    for batch in batch_list:\n",
    "        print(f\"Now doing batch: {batch}\")\n",
    "        image_subset = data[data.obs['batchID'] == batch]\n",
    "        \n",
    "        # change back to one binary one labeled.\n",
    "        # yes binary image is raw\n",
    "        labeled_suffix = '/labeled_image.tif'\n",
    "        binary_suffix = '/binary_image.tif'\n",
    "            \n",
    "        test_im = Mapping.load_tiff_image(root + batch + labeled_suffix)\n",
    "        #raw_im = np.zeros(10)\n",
    "        raw_im = Mapping.load_tiff_image(root + batch + binary_suffix)\n",
    "            \n",
    "        images = load_label_image(test_im, raw_im)\n",
    "        total.append(images)\n",
    "        \n",
    "    print(\"Done Analyzing!\")\n",
    "        \n",
    "    return total\n",
    "\n",
    "def pad_images(image_dict, target_height=800, target_width=800):\n",
    "    \"\"\"\n",
    "    Pad images in the dictionary to the same width and height.\n",
    "    \n",
    "    Parameters:\n",
    "    image_dict : dict\n",
    "        Dictionary where keys are image identifiers and values are numpy arrays representing images.\n",
    "    \n",
    "    Returns:\n",
    "    dict\n",
    "        Dictionary with padded images.\n",
    "    \"\"\"\n",
    "    #max_height = max(image.shape[0] for image in image_dict.values())\n",
    "    #max_width = max(image.shape[1] for image in image_dict.values())\n",
    "\n",
    "    padded_images = {}\n",
    "\n",
    "    for key, image in image_dict.items():\n",
    "        height, width = image.shape\n",
    "        pad_height = target_height - height\n",
    "        pad_width = target_width - width\n",
    "\n",
    "        # Calculate padding for top, bottom, left, and right to center the image\n",
    "        pad_top = pad_height // 2\n",
    "        pad_bottom = pad_height - pad_top\n",
    "        pad_left = pad_width // 2\n",
    "        pad_right = pad_width - pad_left\n",
    "\n",
    "        # Padding: ((top, bottom), (left, right))\n",
    "        padded_image = np.pad(image, ((pad_top, pad_bottom), (pad_left, pad_right)), mode='constant', constant_values=0)\n",
    "\n",
    "        padded_images[key] = padded_image\n",
    "\n",
    "    return padded_images\n",
    "\n",
    "def filter_images(image_dict, max_dim=800):\n",
    "    \"\"\"\n",
    "    Filter images in the dictionary to remove those with width or height over max_dim.\n",
    "    \n",
    "    Parameters:\n",
    "    image_dict : dict\n",
    "        Dictionary where keys are image identifiers and values are numpy arrays representing images.\n",
    "    max_dim : int\n",
    "        Maximum allowed dimension for width and height.\n",
    "    \n",
    "    Returns:\n",
    "    dict\n",
    "        Filtered dictionary.\n",
    "    \"\"\"\n",
    "    filtered_images = {key: image for key, image in image_dict.items() if image.shape[0] <= max_dim and image.shape[1] <= max_dim}\n",
    "    return filtered_images\n",
    "\n",
    "def analyze_image_dimensions(image_dict):\n",
    "    \"\"\"\n",
    "    Analyze the dimensions of images in the dictionary.\n",
    "    \n",
    "    Parameters:\n",
    "    image_dict : dict\n",
    "        Dictionary where keys are image identifiers and values are numpy arrays representing images.\n",
    "    \n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "    widths = [image.shape[1] for image in image_dict.values()]\n",
    "    heights = [image.shape[0] for image in image_dict.values()]\n",
    "\n",
    "    # Create a DataFrame for easy analysis\n",
    "    dimensions_df = pd.DataFrame({'Width': widths, 'Height': heights})\n",
    "\n",
    "    # Print basic statistics\n",
    "    print(dimensions_df.describe())\n",
    "\n",
    "    # Plot the distribution of widths and heights\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "    \n",
    "    axes[0].hist(widths, bins=50, color='skyblue', edgecolor='black')\n",
    "    axes[0].set_title('Distribution of Widths')\n",
    "    axes[0].set_xlabel('Width')\n",
    "    axes[0].set_ylabel('Frequency')\n",
    "    \n",
    "    axes[1].hist(heights, bins=50, color='lightgreen', edgecolor='black')\n",
    "    axes[1].set_title('Distribution of Heights')\n",
    "    axes[1].set_xlabel('Height')\n",
    "    axes[1].set_ylabel('Frequency')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Box plots for additional insights\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "    \n",
    "    sns.boxplot(y=widths, ax=axes[0], color='skyblue')\n",
    "    axes[0].set_title('Box Plot of Widths')\n",
    "    axes[0].set_ylabel('Width')\n",
    "    \n",
    "    sns.boxplot(y=heights, ax=axes[1], color='lightgreen')\n",
    "    axes[1].set_title('Box Plot of Heights')\n",
    "    axes[1].set_ylabel('Height')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "def resize_images(image_dict, target_height=224, target_width=224):\n",
    "    \"\"\"\n",
    "    Pad images in the dictionary to the same width and height.\n",
    "    \n",
    "    Parameters:\n",
    "    image_dict : dict\n",
    "        Dictionary where keys are image identifiers and values are numpy arrays representing images.\n",
    "    \n",
    "    Returns:\n",
    "    dict\n",
    "        Dictionary with padded images.\n",
    "    \"\"\"\n",
    "    #max_height = max(image.shape[0] for image in image_dict.values())\n",
    "    #max_width = max(image.shape[1] for image in image_dict.values())\n",
    "\n",
    "    padded_images = {}\n",
    "\n",
    "    for key, image in image_dict.items():\n",
    "        \n",
    "        padded_images[key] = cv2.resize(image_dict[key], dsize=(target_height,target_width), interpolation=cv2.INTER_NEAREST)\n",
    "\n",
    "    return padded_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be4ec652-0220-428c-91f1-72c3e7ded9ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following functions are directly from the cited repo:\n",
    "\n",
    "# lifted from the cuttlefish paper\n",
    "\n",
    "def apply_clahe(image, clip_limit=2.0, tile_size=(8, 8)):\n",
    "    clahe = cv2.createCLAHE(clipLimit=clip_limit, tileGridSize=tile_size)\n",
    "    return clahe.apply(image)\n",
    "\n",
    "def multi_scale_clahe(image, clip_limit=2.0, tile_sizes=[(8, 8), (16, 16), (32, 32)], weights=None):\n",
    "    if len(image.shape) == 3:\n",
    "        # Convert color image to LAB color space\n",
    "        lab = cv2.cvtColor(image, cv2.COLOR_BGR2LAB)\n",
    "        l, a, b = cv2.split(lab)\n",
    "    else:\n",
    "        l = image  # For grayscale images\n",
    "\n",
    "    # Apply CLAHE at multiple scales\n",
    "    enhanced_ls = [apply_clahe(l, clip_limit, tile_size) for tile_size in tile_sizes]\n",
    "\n",
    "    # Combine the results\n",
    "    if weights is None:\n",
    "        weights = [1/len(enhanced_ls)] * len(enhanced_ls)\n",
    "    \n",
    "    enhanced_l = np.zeros_like(l, dtype=np.float32)\n",
    "    for enhanced, weight in zip(enhanced_ls, weights):\n",
    "        enhanced_l += enhanced * weight\n",
    "    \n",
    "    enhanced_l = np.clip(enhanced_l, 0, 255).astype(np.uint8)\n",
    "\n",
    "    if len(image.shape) == 3:\n",
    "        # Merge back to LAB and convert to BGR\n",
    "        enhanced_lab = cv2.merge((enhanced_l, a, b))\n",
    "        return cv2.cvtColor(enhanced_lab, cv2.COLOR_LAB2BGR)\n",
    "    else:\n",
    "        return enhanced_l\n",
    "\n",
    "def preprocess_image(img,img_width=224,img_height=224):\n",
    "    from tensorflow.keras.applications import vgg19\n",
    "    from keract import get_activations\n",
    "\n",
    "    # from keract import get_activations\n",
    "    from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "    img = img_to_array(img)\n",
    "    img = cv2.resize(img, dsize=(img_width,img_height), interpolation=cv2.INTER_NEAREST)\n",
    "\n",
    "    if len(img.shape)==2:\n",
    "        img = img.reshape((img.shape[0], img.shape[1], 1))\n",
    "        img=np.concatenate((img, img,img),axis=2)\n",
    "\n",
    "    img = img.reshape((1, img.shape[0], img.shape[1], img.shape[2]))\n",
    "    img = img.astype('float32')\n",
    "    img = vgg19.preprocess_input(img)\n",
    "    return img\n",
    "\n",
    "\n",
    "def histN(img):\n",
    "    #img= rgb2gray(img)\n",
    "    img=img.astype(np.uint8)\n",
    "    #img = cv2.equalizeHist(img)\n",
    "    img = multi_scale_clahe(img, \n",
    "                            clip_limit=3.0, \n",
    "                            tile_sizes=[(16, 16), (32, 32), (64, 64)],\n",
    "                            weights=[0.0, 0.0, 1.0])\n",
    "    img = img.reshape((img.shape[0], img.shape[1], 1))\n",
    "    img = np.concatenate((img, img, img), axis=2)\n",
    "    return img\n",
    "def to_tex(img,model,sz=224):\n",
    "    from tensorflow.keras.applications import vgg19\n",
    "    from keract import get_activations\n",
    "    from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "    LAYER = 'block5_conv1'\n",
    "    currImg = preprocess_image(img,sz,sz)\n",
    "    # model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\")\n",
    "    activations = get_activations(model, currImg, layer_names=LAYER, auto_compile=True)[LAYER]#.popitem(True)[1]\n",
    "    # activations = list(get_activations(model, currImg).values())\n",
    "    fifth = activations.max(axis=(1,2))\n",
    "\n",
    "    vggRep = fifth.ravel()\n",
    "    return vggRep\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b18f4486-cc0f-4d05-8f23-bfb86de2e281",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_vectors(adata_filtered, model):\n",
    "    data_list = []\n",
    "\n",
    "    for batch_id in adata_filtered.obs.batchID.unique():\n",
    "        test_im = vectorize_imaging_data(adata_filtered, [batch_id])\n",
    "        test_data = adata_filtered[adata_filtered.obs.batchID == batch_id]\n",
    "        valid_cell_labels = test_data.obs.cell_label.tolist()\n",
    "        cell_dict = test_im[0]\n",
    "        subset_dict = {label: cell_dict[label][0] for label in valid_cell_labels if label in cell_dict}\n",
    "        subset_dict = filter_images(subset_dict, max_dim=800)\n",
    "        filtered_labels = subset_dict.keys()\n",
    "        test_data = test_data[test_data.obs.cell_label.isin(filtered_labels)]\n",
    "        subset_dict = pad_images(subset_dict, target_height=800, target_width=800)\n",
    "\n",
    "        # Generate vectors for each key in subset_dict\n",
    "        for key, value in subset_dict.items():\n",
    "            \n",
    "            currImg = histN(value)\n",
    "            img_res = cv2.resize(currImg, dsize=(112,112), interpolation=cv2.INTER_NEAREST)\n",
    "            padding_size = 56\n",
    "            padding_color_bgr = [103.939, 116.779, 123.68]\n",
    "\n",
    "            padded_image = cv2.copyMakeBorder(\n",
    "                img_res,\n",
    "                padding_size, padding_size, padding_size, padding_size,  # Top, bottom, left, right padding\n",
    "                cv2.BORDER_CONSTANT,\n",
    "                value=padding_color_bgr\n",
    "            )\n",
    "            #img_vgg = preprocess_image(padded_image)\n",
    "            \n",
    "            masked_H = histN(value)\n",
    "            vecRep = to_tex(padded_image, model, 224)\n",
    "            vecRep_serialized = json.dumps(vecRep.tolist())\n",
    "            # Store batchID, key, and vector in the list\n",
    "            data_list.append({'batchID': batch_id, 'key': key, 'vector': vecRep_serialized})\n",
    "\n",
    "    # Convert the list to a DataFrame\n",
    "    df = pd.DataFrame(data_list)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e08e5214-0cb1-481d-82bc-19121c8781b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "adata_filtered = sc.read_h5ad('Microglia_Shapespace_baysor_500.h5ad')\n",
    "\n",
    "model = vgg19.VGG19(weights='imagenet',include_top=False)\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee4eb2e1-d228-4fae-8ef3-ad88557a0ba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_space = generate_vectors(adata_filtered, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5065d053-facb-4378-af8f-3a5639014235",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_vectors = feature_space.copy()\n",
    "\n",
    "df_vectors.rename(columns={'key': 'cell_label'}, inplace=True)\n",
    "\n",
    "# Merge the DataFrame with adata_filtered.obs on 'batchID' and 'cell_label'\n",
    "merged_df = adata_filtered.obs.reset_index().merge(df_vectors, on=['batchID', 'cell_label'], how='left').set_index('index')\n",
    "\n",
    "# Add the aligned vectors to adata_filtered.obs\n",
    "adata_filtered.obs['texture'] = merged_df['vector']\n",
    "\n",
    "adata_filtered.write_h5ad('Microglia_cuttlefish_500_block5_conv1.h5ad')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baf25201-815d-4059-9ad1-ef2863a12504",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we can go ahead and create a umap for this:\n",
    "\n",
    "adata = sc.read_h5ad('Microglia_cuttlefish_500_block5_conv1.h5ad')\n",
    "adata = adata[~adata.obs['texture'].isna()].copy()\n",
    "\n",
    "vector_list = [json.loads(vec) if pd.notnull(vec) else np.zeros(512) for vec in adata.obs['texture']]\n",
    "vector_matrix = np.array(vector_list)\n",
    "\n",
    "# Step 2: Perform PCA\n",
    "pca = PCA(n_components=10)\n",
    "principal_components = pca.fit_transform(vector_matrix)\n",
    "\n",
    "# Step 3: Calculate UMAP\n",
    "n_neighbors = 10\n",
    "umap_model = umap.UMAP(n_neighbors=n_neighbors)\n",
    "umap_embedding = umap_model.fit_transform(principal_components)\n",
    "\n",
    "adata.obsm['X_pca'] = principal_components\n",
    "\n",
    "# Step 4: Add UMAP embedding to AnnData object\n",
    "adata.obsm['X_umap_shapes'] = umap_embedding\n",
    "\n",
    "# Example of how to plot UMAP embedding using scanpy, colored by batchID\n",
    "sc.pl.embedding(adata, basis='X_umap_shapes', color=['batchID', 'Age'], title='Custom UMAP of Microglia')\n",
    "\n",
    "adata.write_h5ad('Microglia_cuttlefish_500_block5_conv1_umap.h5ad')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Vizgen_2",
   "language": "python",
   "name": "vizgen_2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
