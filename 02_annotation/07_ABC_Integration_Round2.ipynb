{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c8285725-de55-44d4-b317-c7b64f91faea",
   "metadata": {},
   "source": [
    "# This notebook must be run with the ABC_Download conda environment within the ABC.sif singularity container"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acddf1fb-25d4-4c6a-b91e-b19a9f6467ba",
   "metadata": {},
   "source": [
    "This code, and the following two notebooks, are largely inspired by the Allen Brain Cell Atlas's MERFISH atlas. The code used was sourced and adapted from this github: https://github.com/ZhuangLab/whole_mouse_brain_MERFISH_atlas_scripts_2023/blob/main/scripts/integrate_MERFISH_with_scRNA-seq/integration_round2.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "46b45cbf-f34a-4bd2-af89-300496390cae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import numpy as np\n",
    "import scipy.sparse\n",
    "import scipy.spatial\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import anndata\n",
    "import scanpy as sc\n",
    "sc.settings.n_jobs = 56\n",
    "sc.settings.set_figure_params(dpi=180, dpi_save=300, frameon=False, figsize=(4, 4), fontsize=8, facecolor='white')\n",
    "\n",
    "import ALLCools\n",
    "from ALLCools.integration.seurat_class import SeuratIntegration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "13425b70-7906-4370-9e06-f9e84231bb53",
   "metadata": {},
   "outputs": [],
   "source": [
    "workspace_path = '/hpc/projects/group.quake/doug/references/ABC/whole/integration_workspace'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d6e73254-96fd-443a-9de5-5c6a14348a69",
   "metadata": {},
   "outputs": [],
   "source": [
    "def impute_gene_expression(integrator, adata_list, X_ref, ref=0, qry=1, npc=30, kweight=30, sd=1,\n",
    "                           chunk_size=1000, random_state=0):\n",
    "    data_qry = adata_list[qry].obsm['X_pca']\n",
    "\n",
    "    anchor, G, D, cum_qry = integrator.find_nearest_anchor(adata_list, data_qry=data_qry, ref=[ref], qry=[qry],\n",
    "                                                      npc=npc, k_weight=kweight, sd=sd, random_state=random_state)\n",
    "    \n",
    "    if scipy.sparse.issparse(X_ref):\n",
    "        X_ref = X_ref.toarray()\n",
    "    \n",
    "    X_anchor = X_ref[anchor[:, 0]]\n",
    "    imputed_chunks = []\n",
    "    \n",
    "    for chunk_start in np.arange(0, data_qry.shape[0], chunk_size):\n",
    "        imputed_chunks.append(scipy.sparse.csr_matrix((\n",
    "                        D[chunk_start:(chunk_start + chunk_size), :, None] *\n",
    "                        X_anchor[G[chunk_start:(chunk_start + chunk_size)]]\n",
    "                    ).sum(axis=1).astype(np.float32)))\n",
    "\n",
    "    return scipy.sparse.vstack(imputed_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2bf4243-b312-4a54-86de-e557c24895c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is going to remain unrun so that it can be stored in .git\n",
    "%%time\n",
    "imputation_path = os.path.join(workspace_path, 'gene_expression_imputation')\n",
    "partition_path = os.path.join(workspace_path, 'partitions')\n",
    "partitions = sorted(os.listdir(partition_path))\n",
    "\n",
    "for pn in partitions:\n",
    "    print(f'Integrate for {pn}')\n",
    "    integration_path = os.path.join(partition_path, pn)\n",
    "    input_adata_seq_file = os.path.join(integration_path, 'adata_seq.h5ad')\n",
    "    input_adata_merfish_file = os.path.join(integration_path, 'adata_merfish_integrated.h5ad')\n",
    "    \n",
    "    if not (os.path.exists(input_adata_seq_file) and os.path.exists(input_adata_merfish_file)):\n",
    "        print('Missing input file. Skip the integration.')\n",
    "        continue\n",
    "        \n",
    "    ## Load and preprocess the data\n",
    "    adata_seq_raw = sc.read_h5ad(input_adata_seq_file)\n",
    "    adata_seq = adata_seq_raw.copy()\n",
    "    adata_merfish_raw = sc.read_h5ad(input_adata_merfish_file)\n",
    "    adata_merfish = adata_merfish_raw.copy()\n",
    "    adata_merfish.X = adata_merfish.layers['counts']\n",
    "\n",
    "    # Consider the common genes\n",
    "    common_genes = np.array(adata_seq.var_names.intersection(adata_merfish.var_names))\n",
    "    adata_seq._inplace_subset_var(common_genes)\n",
    "    \n",
    "    # Normalize\n",
    "    sc.pp.normalize_total(adata_seq, target_sum=1000)\n",
    "\n",
    "    sc.pp.log1p(adata_seq)\n",
    "\n",
    "    sc.pp.normalize_total(adata_merfish, target_sum=1000)\n",
    "\n",
    "    sc.pp.log1p(adata_merfish)\n",
    "    \n",
    "    # Select variable genes\n",
    "    sc.pp.highly_variable_genes(adata_seq)\n",
    "    hv_genes = list(adata_seq.var.index[adata_seq.var['dispersions'] > 0])\n",
    "    adata_seq = adata_seq[:, adata_seq.var.index.isin(hv_genes)]\n",
    "    adata_merfish = adata_merfish[:, adata_merfish.var.index.isin(hv_genes)]\n",
    "    print(f'Use {len(hv_genes)} highly variable genes for integration.')\n",
    "    \n",
    "    # Scale the data\n",
    "\n",
    "    sc.pp.scale(adata_seq)\n",
    "\n",
    "    sc.pp.scale(adata_merfish)\n",
    "    \n",
    "    # Merge the datasets\n",
    "    adata_merge = adata_seq.concatenate(adata_merfish,\n",
    "                                    batch_categories=['seq', 'merfish'],\n",
    "                                    batch_key='modality',\n",
    "                                    index_unique=None)\n",
    "    \n",
    "    # PCA\n",
    "    n_pcs = min(100, len(hv_genes) - 1)\n",
    "\n",
    "    sc.tl.pca(adata_merge, svd_solver='arpack', n_comps=n_pcs)\n",
    "    \n",
    "    adata_list = [adata_merge[adata_merge.obs['modality'] == 'seq'],\n",
    "                  adata_merge[adata_merge.obs['modality'] == 'merfish']]\n",
    "    \n",
    "    ## Integration\n",
    "    # Find the integration anchors\n",
    "\n",
    "    integrator = SeuratIntegration()\n",
    "    integrator.find_anchor(adata_list,\n",
    "                       k_local=None,\n",
    "                       key_local='X_pca',\n",
    "                       k_anchor=5,\n",
    "                       key_anchor='X',\n",
    "                       dim_red='cca',\n",
    "                       max_cc_cells=100000,\n",
    "                       k_score=30,\n",
    "                       k_filter=None, #why?\n",
    "                       scale1=False,\n",
    "                       scale2=False,\n",
    "                       n_components=n_pcs,\n",
    "                       n_features=200,\n",
    "                       alignments=[[[0], [1]]])\n",
    "    \n",
    "    # Label transfer\n",
    "    #cell_type_cols = ['Level1_label', 'Level2_label', 'cl']\n",
    "    cell_type_cols = ['subclass_label', 'cl']\n",
    "    transfer_results = integrator.label_transfer(\n",
    "        ref=[0],\n",
    "        qry=[1],\n",
    "        categorical_key=cell_type_cols,\n",
    "        key_dist='X_pca',\n",
    "        k_weight=30,\n",
    "        npc=n_pcs\n",
    "    )\n",
    "    integrator.save_transfer_results_to_adata(adata_merge, transfer_results)\n",
    "    \n",
    "    # Assign the transfered labels and the confidence\n",
    "    for cell_type_col in cell_type_cols:\n",
    "        adata_merfish_raw.obs[cell_type_col + '_transfer'] = transfer_results[cell_type_col].idxmax(axis=1\n",
    "                                                                                           ).astype('category')\n",
    "        adata_merfish_raw.obs[cell_type_col + '_confidence'] = transfer_results[cell_type_col].max(axis=1)\n",
    "        \n",
    "        n_transfered = len(np.unique(adata_merfish_raw.obs[cell_type_col + '_transfer']))\n",
    "        n_total = len(np.unique(adata_merge.obs[cell_type_col + '_transfer']))\n",
    "        print(f'Transfered {n_transfered}/{n_total} {cell_type_col}.')\n",
    "    \n",
    "    # Save the label transfer results\n",
    "    adata_merfish_raw.write_h5ad(os.path.join(integration_path, 'adata_merfish_label_transfer.h5ad'), \n",
    "                                 compression='gzip')     \n",
    "    \n",
    "    # Impute gene expression\n",
    "    X_imputed = impute_gene_expression(integrator, adata_list, adata_seq_raw.X, \n",
    "                                       ref=0, qry=1, npc=n_pcs, chunk_size=5000)\n",
    "    adata_imputed = anndata.AnnData(X=X_imputed, obs=adata_merfish_raw.obs.copy(), \n",
    "                                var=adata_seq_raw.var.copy(), dtype=np.float32)\n",
    "    \n",
    "    # Create the imputation gene partition table\n",
    "    gene_partition_file = os.path.join(imputation_path, 'gene_partition.csv')\n",
    "    if not os.path.exists(gene_partition_file):\n",
    "        gene_partition_df = adata_seq_raw.var.copy()\n",
    "        gene_partition_df.index.name = 'gene'\n",
    "        gene_partition_df['gene_partition'] = np.arange(gene_partition_df.shape[0], dtype=int) // 1000\n",
    "        gene_partition_df.to_csv(gene_partition_file)\n",
    "        \n",
    "    else:\n",
    "        gene_partition_df = pd.read_csv(gene_partition_file).set_index('gene')\n",
    "    \n",
    "    gene_partitions = np.unique(gene_partition_df['gene_partition'])\n",
    "    \n",
    "    # Save the imputation results\n",
    "    for sn in np.unique(adata_imputed.obs['subclass_label_transfer']):\n",
    "        adata_subset = adata_imputed[adata_imputed.obs['subclass_label_transfer'] == sn]\n",
    "        \n",
    "        os.makedirs(os.path.join(imputation_path, sn.replace('/', '-').replace(' ', '_')), exist_ok=True)\n",
    "        for g_p in gene_partitions:\n",
    "            p_genes = np.array(gene_partition_df[gene_partition_df['gene_partition'] == g_p].index)\n",
    "            adata_ct_p = adata_subset[:, adata_subset.var.index.isin(p_genes)]\n",
    "            adata_ct_p.write_h5ad(os.path.join(imputation_path, sn.replace('/', '-').replace(' ', '_'), \n",
    "                                               f'{g_p}.h5ad'), compression='gzip')\n",
    "\n",
    "        \n",
    "    ## Co-embedding\n",
    "    # Correct the PCs using the integration anchors\n",
    "    corrected = integrator.integrate(key_correct='X_pca',\n",
    "                                 row_normalize=True,\n",
    "                                 n_components=n_pcs,\n",
    "                                 k_weight=100,\n",
    "                                 sd=1,\n",
    "                                 alignments=[[[0], [1]]])\n",
    "\n",
    "    adata_merge.obsm['X_pca_integrate'] = np.concatenate(corrected)\n",
    "    \n",
    "    # Calculate KNN using the integrated PCs\n",
    "    sc.pp.neighbors(adata_merge, use_rep='X_pca_integrate')\n",
    "    \n",
    "    if len(np.unique(adata_merge.obs['subclass_label_transfer'])) > 1:\n",
    "        # Generate the PAGA plot for the initial arrangement of the UMAP\n",
    "        sc.tl.paga(adata_merge, groups='subclass_label_transfer')\n",
    "        sc.pl.paga(adata_merge, save='_tmp.png', cmap='gist_ncar')\n",
    "        shutil.move('figures/paga_tmp.png', os.path.join(integration_path, 'integration_paga_round2.png'))\n",
    "    \n",
    "        sc.tl.umap(adata_merge, init_pos='paga', min_dist=0.5)\n",
    "    else:\n",
    "        sc.tl.umap(adata_merge, min_dist=0.5)\n",
    "        \n",
    "    # Save the umap\n",
    "    sc.pl.umap(adata_merge, color='modality', save='_tmp.png')\n",
    "    shutil.move('figures/umap_tmp.png', os.path.join(integration_path, 'integration_umap_modality.png'))\n",
    "    #sc.pl.umap(adata_merge, color='Level1_label_transfer', save='_tmp.png')\n",
    "    #shutil.move('figures/umap_tmp.png', os.path.join(integration_path, 'integration_umap_Level1_label.png'))\n",
    "    sc.pl.umap(adata_merge, color='subclass_label_transfer', save='_tmp.png', palette='gist_ncar')\n",
    "    shutil.move('figures/umap_tmp.png', os.path.join(integration_path, 'integration_umap_subclass_label.png'))\n",
    "    sc.pl.umap(adata_merge, color='cl_transfer', save='_tmp.png', palette='gist_ncar')\n",
    "    shutil.move('figures/umap_tmp.png', os.path.join(integration_path, 'integration_umap_cl.png'))\n",
    "    \n",
    "    \n",
    "    # Save the merged adata\n",
    "    adata_merge.write_h5ad(os.path.join(integration_path, 'adata_merged_round2.h5ad'), compression='gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d8c308a2-8b28-4484-b9c5-9cfd9f68baa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cluster_mean_expression_matrix(adata, cluster_column):\n",
    "    '''Get a dataframe of mean gene expression of each cluster.'''\n",
    "    if scipy.sparse.issparse(adata.X):\n",
    "        X = adata.X.toarray()\n",
    "    else:\n",
    "        X = adata.X\n",
    "   \n",
    "    cell_exp_mtx = pd.DataFrame(X, index=adata.obs[cluster_column], columns=adata.var.index)    \n",
    "    return cell_exp_mtx.groupby(by=cluster_column).mean()\n",
    "\n",
    "\n",
    "def calc_cell_cosines_to_cluster_mean_exps(adata, cluster_col, cluster_mean_exp_df):\n",
    "    cluster_mean_exp_df = cluster_mean_exp_df.loc[:, adata.var.index]\n",
    "    cluster_id_map = {cluster_mean_exp_df.index[i]:i for i in range(cluster_mean_exp_df.shape[0])}\n",
    "    cell_cluster_ids = np.array(adata.obs[cluster_col].map(cluster_id_map))\n",
    "    \n",
    "    X_cluster_mean = cluster_mean_exp_df.values[cell_cluster_ids]\n",
    "    \n",
    "    if scipy.sparse.issparse(adata.X):\n",
    "        X = adata.X.toarray()\n",
    "    else:\n",
    "        X = adata.X\n",
    "    \n",
    "    cosines = []\n",
    "    for i in range(adata.shape[0]):\n",
    "        if np.sum(X[i]) > 0:\n",
    "            cosines.append(1 - scipy.spatial.distance.cosine(X[i], X_cluster_mean[i]))\n",
    "        else:\n",
    "            cosines.append(0)\n",
    "    \n",
    "    return cosines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c863ed4e-d985-4007-8682-f7ee4e65093f",
   "metadata": {},
   "outputs": [],
   "source": [
    "adatas = []\n",
    "for pn in partitions:\n",
    "    adata_file = os.path.join(partition_path, pn, 'adata_merfish_label_transfer.h5ad')\n",
    "    adatas.append(sc.read_h5ad(adata_file))\n",
    "    \n",
    "adata = anndata.concat(adatas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aa7e4dea-4ec1-4a21-97fc-b6f2eabd48a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/doug.henze/.conda/envs/ABC_Download/lib/python3.9/site-packages/anndata/_core/anndata.py:1818: UserWarning: Observation names are not unique. To make them unique, call `.obs_names_make_unique`.\n",
      "  utils.warn_names_duplicates(\"obs\")\n",
      "/tmp/ipykernel_256262/820537460.py:9: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  return cell_exp_mtx.groupby(by=cluster_column).mean()\n",
      "/home/doug.henze/.conda/envs/ABC_Download/lib/python3.9/site-packages/scanpy/preprocessing/_normalization.py:234: UserWarning: Some cells have zero counts\n",
      "  warn(UserWarning(\"Some cells have zero counts\"))\n",
      "/home/doug.henze/.conda/envs/ABC_Download/lib/python3.9/site-packages/scanpy/preprocessing/_simple.py:384: RuntimeWarning: divide by zero encountered in log1p\n",
      "  np.log1p(X, out=X)\n",
      "/home/doug.henze/.conda/envs/ABC_Download/lib/python3.9/site-packages/scanpy/preprocessing/_simple.py:384: RuntimeWarning: invalid value encountered in log1p\n",
      "  np.log1p(X, out=X)\n"
     ]
    }
   ],
   "source": [
    "# Get the mean expressions of each cluster\n",
    "adata_seq = sc.read_h5ad(os.path.join(workspace_path, 'adata_seq_common_genes.h5ad'))\n",
    "sc.pp.normalize_total(adata_seq, target_sum=1000)\n",
    "sc.pp.log1p(adata_seq)\n",
    "\n",
    "cluster_mean_exp_df = get_cluster_mean_expression_matrix(adata_seq, 'cl')\n",
    "\n",
    "# Get the MERFISH log1p expressions\n",
    "adata_merfish_log1p = adata.copy()\n",
    "sc.pp.normalize_total(adata_merfish_log1p, target_sum=1000)\n",
    "sc.pp.log1p(adata_merfish_log1p)\n",
    "\n",
    "# Calcualte the cluster cosine similarities\n",
    "adata.obs['cluster_cosine_similarity'] = calc_cell_cosines_to_cluster_mean_exps(adata_merfish_log1p, \n",
    "                                                    'cl_transfer', cluster_mean_exp_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b98c7125-d49d-4fb0-9e85-aece0e9a4f7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "adata.obs['adjusted_subclass_label_confidence'] = (adata.obs['integration_partition_confidence'] \n",
    "                                                        * adata.obs['subclass_label_confidence'])\n",
    "adata.obs['adjusted_cl_confidence'] = adata.obs['integration_partition_confidence'] * adata.obs['cl_confidence']\n",
    "\n",
    "\n",
    "adata.write_h5ad(os.path.join(workspace_path, 'adata_merfish_label_transfer.h5ad'), compression='gzip')\n",
    "adata.obs.to_csv(os.path.join(workspace_path, 'adata_merfish_label_transfer_metadata.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "434abdd5-6165-488c-987d-8cfd97002d8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "adata.write_h5ad('ABC_transferred.h5ad')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ABC",
   "language": "python",
   "name": "abc"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
