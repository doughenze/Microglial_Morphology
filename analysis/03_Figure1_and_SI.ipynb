{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9cb69df-a0dc-4eff-868b-cfe52484ce90",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scanpy as sc\n",
    "import pandas as pd\n",
    "import os\n",
    "import string\n",
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "from shapely.geometry import Point\n",
    "from tqdm import tqdm\n",
    "import Mapping\n",
    "import matplotlib.pyplot as plt\n",
    "import multiprocessing as mp\n",
    "from functools import partial\n",
    "\n",
    "import skimage\n",
    "import cv2\n",
    "from skimage.morphology import disk, opening, closing\n",
    "from scipy.ndimage import binary_fill_holes, label, distance_transform_edt\n",
    "from skimage.segmentation import find_boundaries, watershed\n",
    "from skimage.feature import peak_local_max\n",
    "from skimage.measure import regionprops\n",
    "from skimage.segmentation import watershed\n",
    "import anndata\n",
    "from adjustText import adjust_text\n",
    "\n",
    "import Mapping\n",
    "import os\n",
    "import string\n",
    "\n",
    "import cv2\n",
    "import geopandas as gpd\n",
    "import igraph as ig\n",
    "#import leidenalg\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import rasterio\n",
    "import seaborn as sns\n",
    "import tifffile\n",
    "import umap\n",
    "from anndata import AnnData as ad\n",
    "from matplotlib import patches as mpatches\n",
    "#from matplotlib_scalebar.scalebar import ScaleBar\n",
    "import scanpy as sc\n",
    "import anndata\n",
    "from shapely.affinity import translate\n",
    "from shapely.geometry import Polygon, MultiPolygon, box, shape\n",
    "from skimage import img_as_bool, img_as_ubyte\n",
    "from skimage.measure import find_contours, regionprops, regionprops_table\n",
    "from skimage.morphology import skeletonize, opening\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.neighbors import kneighbors_graph\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from skimage.segmentation import find_boundaries\n",
    "from tqdm import tqdm\n",
    "import matplotlib.patches as patches\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e5d2915-e628-4e5b-8feb-dbf0ccbdccb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "ad_parent = sc.read_h5ad('Shape_500.h5ad')\n",
    "ad_parent = ad_parent[ad_parent.obs.updated_celltype == 'Microglia']\n",
    "ad_parent.X = ad_parent.layers['counts'].todense().copy()\n",
    "gene_counts_per_cell = np.count_nonzero(ad_parent.X, axis=1)\n",
    "ad_parent.obs['gene_counts'] = gene_counts_per_cell\n",
    "\n",
    "ad_list = []\n",
    "for batch in ad_parent.obs.batchID.unique():\n",
    "    ad_viz = ad_parent[ad_parent.obs.batchID == batch].copy()\n",
    "    \n",
    "    total = pd.read_csv(f\"transcript_out/{batch}_nuc_y_non_nuc.csv\",index_col=0)\n",
    "    non_nuc = pd.read_csv(f\"transcript_out/{batch}_non_nuc.csv\",index_col=0)\n",
    "    \n",
    "    ad_viz.layers['total_counts'] = total[ad_viz.var_names.intersection(total.columns)].to_numpy()\n",
    "    ad_viz.layers['non_nuc_counts'] = non_nuc[ad_viz.var_names.intersection(non_nuc.columns)].to_numpy()\n",
    "    \n",
    "    ad_viz.layers['nuc_counts'] = ad_viz.layers['total_counts'] - ad_viz.layers['non_nuc_counts']\n",
    "    \n",
    "    ad_list.append(ad_viz)\n",
    "    \n",
    "combined_adata = anndata.concat(ad_list, axis=0)\n",
    "\n",
    "adata = combined_adata.copy()\n",
    "adata.X = adata.layers['total_counts'].copy()\n",
    "sc.pp.normalize_total(adata,target_sum=1e4)\n",
    "sc.pp.log1p(adata)\n",
    "\n",
    "adata.raw = adata\n",
    "sc.pp.scale(adata, max_value=10)\n",
    "sc.tl.pca(adata, svd_solver=\"arpack\")\n",
    "sc.pp.neighbors(adata, n_neighbors=10, n_pcs=40)\n",
    "sc.tl.umap(adata,random_state=0)\n",
    "sc.tl.leiden(adata,resolution=0.6,random_state=42)\n",
    "sc.pl.umap(adata, color=[\"morph_leiden\",\"leiden\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12ed47c4-4b45-4b62-8587-4d63cc12b350",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_ad = adata.copy()\n",
    "new_ad.X = new_ad.layers['total_counts'].copy()\n",
    "sc.pp.normalize_total(new_ad, target_sum=1e4)\n",
    "sc.pp.log1p(new_ad)\n",
    "\n",
    "clusters = {\n",
    "    '0':'Homeostatic',\n",
    "    '1':'Transitioning',\n",
    "    '2':'DAM',\n",
    "    '3':'Transitioning',\n",
    "    '4':'Homeostatic',\n",
    "    '5':'Transitioning',\n",
    "    '6':'Transitioning',\n",
    "    '7':'Homeostatic',\n",
    "}\n",
    "new_ad.obs['sub_mic'] = new_ad.obs['leiden'].map(clusters)\n",
    "\n",
    "sc.pl.matrixplot(new_ad,groupby='sub_mic',standard_scale='group',cmap='coolwarm',var_names=['Itgax','Spp1','Lpl','Apoe','Axl','P2ry12','P2ry13','Cx3cr1','Tmem119','Csf1r'\n",
    "                                                                         ,'Maf','Prkca','Stab1','Cd83','Hif1a'], save='_clusters.pdf')\n",
    "\n",
    "sc.pl.umap(new_ad, color=[\"Apoe\",\"Tmem119\",\"Hif1a\"],cmap='hot',save='_hot_genes.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e77631fd-2254-4f15-8096-f39d6b0075f2",
   "metadata": {},
   "source": [
    "# Example Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1e478ec-82dc-41de-9232-1aad0e3040ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "mosaic_to_micron = pd.read_csv('/3-mo-female-1-rev2/images/micron_to_mosaic_pixel_transform.csv',delimiter=' ').iloc[0,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "947687c2-5f85-4098-9f47-4d49479cd95e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_random_cells_from_cluster(data, cluster_id,column='leiden', num_cells=9,raw=False,random_state=35, conversion_rate=1.0, micron_length=50, output_pdf=None):\n",
    "    \"\"\"\n",
    "    Plot random cells from the specified Leiden cluster.\n",
    "    \n",
    "    Args:\n",
    "    - data (pd.DataFrame): DataFrame containing 'batchID', 'cell_label', and 'leiden_cluster'.\n",
    "    - cluster_id (int): The Leiden cluster ID to visualize.\n",
    "    - num_cells (int): Number of cells to display, default is 4.\n",
    "    \"\"\"\n",
    "    # Filter data for the specified cluster\n",
    "    cluster_data = data[data[column] == cluster_id]\n",
    "    #print(cluster_data)\n",
    "    \n",
    "    # Randomly select cells if there are more than num_cells\n",
    "    if len(cluster_data) > num_cells:\n",
    "        cluster_data = cluster_data.sample(n=num_cells, random_state=random_state)\n",
    "    \n",
    "    # Create a grid plot\n",
    "    fig, axes = plt.subplots(nrows=3, ncols=3, figsize=(15, 15))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    for ax, (_, row) in zip(axes, cluster_data.iterrows()):\n",
    "        print(row['batchID'])\n",
    "        image = load_label_image(row['batchID'], row['x'], row['y'], raw)\n",
    "        ax.imshow(image, cmap='gray')\n",
    "        ax.axis('off')\n",
    "        ax.set_title(f\"Batch: {row['batchID']}, x: {row['x']}, y:{row['y']}\")\n",
    "        add_scale_bar(ax, conversion_rate, micron_length, location=(10, image.shape[0] - 20))\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if output_pdf:\n",
    "        plt.savefig(output_pdf, format='pdf', bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7e39e08-8771-4d47-b786-1ea8cef00c3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_random_cells_from_cluster(data, cluster_id,column='leiden', num_cells=9,raw=False,random_state=35, conversion_rate=1.0, micron_length=50, output_pdf=None):\n",
    "    \"\"\"\n",
    "    Plot random cells from the specified Leiden cluster.\n",
    "    \n",
    "    Args:\n",
    "    - data (pd.DataFrame): DataFrame containing 'batchID', 'cell_label', and 'leiden_cluster'.\n",
    "    - cluster_id (int): The Leiden cluster ID to visualize.\n",
    "    - num_cells (int): Number of cells to display, default is 4.\n",
    "    \"\"\"\n",
    "    # Filter data for the specified cluster\n",
    "    cluster_data = data[data[column] == cluster_id]\n",
    "    #print(cluster_data)\n",
    "    \n",
    "    # Randomly select cells if there are more than num_cells\n",
    "    if len(cluster_data) > num_cells:\n",
    "        cluster_data = cluster_data.sample(n=num_cells, random_state=random_state)\n",
    "    \n",
    "    # Create a grid plot\n",
    "    fig, axes = plt.subplots(nrows=3, ncols=3, figsize=(15, 15))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    for ax, (_, row) in zip(axes, cluster_data.iterrows()):\n",
    "        print(row['batchID'])\n",
    "        image = load_label_image(row['batchID'], row['x'], row['y'], raw)\n",
    "        ax.imshow(image, cmap='gray')\n",
    "        ax.axis('off')\n",
    "        ax.set_title(f\"Batch: {row['batchID']}, x: {row['x']}, y:{row['y']}\")\n",
    "        add_scale_bar(ax, conversion_rate, micron_length, location=(10, image.shape[0] - 20))\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if output_pdf:\n",
    "        plt.savefig(output_pdf, format='pdf', bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65d0f30e-6579-4af0-90df-84e720599b9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_random_cells_from_cluster(new_ad.obs, cluster_id='DAM', column='sub_mic', raw=True,random_state=10,conversion_rate=mosaic_to_micron,output_pdf='figures/DAM_examples.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "923e9682-1d12-47e1-8eeb-2a21d53fe158",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_random_cells_from_cluster(new_ad.obs, cluster_id='Homeostatic', column='sub_mic', raw=True,conversion_rate=mosaic_to_micron,output_pdf='figures/Homeostatic_examples.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea9eb72e-ea9d-45d7-8230-b0d74fd1dd08",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_random_cells_from_cluster(new_ad.obs, cluster_id='Transitioning', column='sub_mic', raw=True,conversion_rate=mosaic_to_micron,output_pdf='figures/Transitioning_examples.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbf71bc5-87ad-469d-b62c-dd1bfec24a37",
   "metadata": {},
   "outputs": [],
   "source": [
    "place_in_order = {\n",
    "    0: '2',\n",
    "    1: '3',\n",
    "    2: '1',\n",
    "    3: '0',\n",
    "    4: '4',\n",
    "}\n",
    "\n",
    "new_ad.obs['ordered_morph'] = new_ad.obs.morph_leiden.map(place_in_order)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac1a2fc0-ba84-4df3-815a-0975c3e379d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_ad.write_h5ad('Transciptomic_labels_and_morphology_labels_full.h5ad')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54e0dcb7-7aa7-4e93-9da1-e76523cb2ee6",
   "metadata": {},
   "source": [
    "# Supplemental Figure 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "544053d3-7cc3-4ce1-b8ac-90c4006b5e06",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scanpy as sc\n",
    "import pandas as pd\n",
    "import os\n",
    "import string\n",
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import Mapping\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import patches as mpatches\n",
    "import multiprocessing as mp\n",
    "from functools import partial\n",
    "import seaborn as sns\n",
    "\n",
    "import skimage\n",
    "import cv2\n",
    "from skimage.morphology import disk, opening, closing\n",
    "from scipy.ndimage import binary_fill_holes, label, distance_transform_edt\n",
    "from skimage.segmentation import find_boundaries, watershed\n",
    "from skimage.feature import peak_local_max\n",
    "from skimage.measure import regionprops, block_reduce\n",
    "\n",
    "import anndata\n",
    "from adjustText import adjust_text\n",
    "\n",
    "from shapely.geometry import Polygon, shape, box, Point, MultiPolygon\n",
    "from shapely.affinity import affine_transform, translate\n",
    "import geopandas as gpd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ac65314-c0c4-4886-8564-db0326ec737f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_filtered_transcripts(experiment_path):\n",
    "    region_types = ['region_0', 'region_1']\n",
    "    for region in region_types:\n",
    "        file_path = f'{experiment_path}baysor/detected_transcripts.csv'\n",
    "        if os.path.exists(file_path):\n",
    "            return pd.read_csv(file_path,index_col=0)\n",
    "    return None\n",
    "\n",
    "def extract_sub_image_with_padding(image, bbox, padding=10):\n",
    "    min_row, min_col, max_row, max_col = bbox\n",
    "    min_row = max(min_row - padding, 0)\n",
    "    min_col = max(min_col - padding, 0)\n",
    "    max_row = min(max_row + padding, image.shape[0])\n",
    "    max_col = min(max_col + padding, image.shape[1])\n",
    "    return image[min_row:max_row, min_col:max_col], (min_row, min_col)\n",
    "\n",
    "def load_images(batchID, x_ax, y_ax, raw_im, raw_dapi,transcripts, geometries,save_path=None):\n",
    "    root = '/hpc/projects/group.quake/doug/Shapes_Spatial/'\n",
    "    \n",
    "    transform_file = f'{root}{batchID}/images/micron_to_mosaic_pixel_transform.csv'\n",
    "    transform_df = pd.read_table(transform_file, sep=' ', header=None)\n",
    "    transformation_matrix = transform_df.values\n",
    "    \n",
    "    x_ax = round(x_ax * transformation_matrix[0, 0] + transformation_matrix[0, 2])\n",
    "    y_ax = round(y_ax * transformation_matrix[1, 1] + transformation_matrix[1, 2])\n",
    "    \n",
    "    #print(f'load {batchID}')\n",
    "    #raw_im = Mapping.load_tiff_image(root + batchID + '/binary_image.tif')\n",
    "    #dapi_im = Mapping.load_tiff_image(root + batchID + '/images/mosaic_DAPI_z3.tif')\n",
    "    \n",
    "    box_size = 500\n",
    "    x_start = x_ax - box_size\n",
    "    x_end = x_ax + box_size\n",
    "    y_start = y_ax - box_size\n",
    "    y_end = y_ax + box_size\n",
    "    scaled_geometries = geometries\n",
    "    \n",
    "    # Create a box for the small image bounding box\n",
    "    small_image_box = box(x_start, y_start, x_end, y_end)\n",
    "    \n",
    "    # Clip geometries to the bounding box of the small image\n",
    "    clipped_geometries = [g.intersection(small_image_box) for g in scaled_geometries if g.intersects(small_image_box)]  \n",
    "    \n",
    "    # Extract the sub-image, ensuring the indices are within bounds\n",
    "    sub_image = np.zeros((2 * box_size, 2 * box_size), dtype=raw_im.dtype)\n",
    "    sub_dapi = np.zeros((2 * box_size, 2 * box_size), dtype=raw_dapi.dtype)\n",
    "    \n",
    "    raw_x_start = max(x_start, 0)\n",
    "    raw_x_end = min(x_end, raw_im.shape[1])\n",
    "    raw_y_start = max(y_start, 0)\n",
    "    raw_y_end = min(y_end, raw_im.shape[0])\n",
    "    \n",
    "    sub_x_start = max(0, -x_start)\n",
    "    sub_x_end = sub_x_start + (raw_x_end - raw_x_start)\n",
    "    sub_y_start = max(0, -y_start)\n",
    "    sub_y_end = sub_y_start + (raw_y_end - raw_y_start)\n",
    "    \n",
    "    sub_image[sub_y_start:sub_y_end, sub_x_start:sub_x_end] = raw_im[raw_y_start:raw_y_end, raw_x_start:raw_x_end]\n",
    "    sub_dapi[sub_y_start:sub_y_end, sub_x_start:sub_x_end] = raw_dapi[raw_y_start:raw_y_end, raw_x_start:raw_x_end]\n",
    "    \n",
    "    transcripts_sub = transcripts.loc[\n",
    "        (transcripts.mosaic_x < raw_x_end) & (transcripts.mosaic_x > raw_x_start) &\n",
    "        (transcripts.mosaic_y > raw_y_start) & (transcripts.mosaic_y < raw_y_end)\n",
    "    ].copy()  # Explicitly create a copy\n",
    "\n",
    "    # Now assign the new values without triggering the warning\n",
    "    transcripts_sub['translate_x'] = transcripts_sub.mosaic_x - raw_x_start\n",
    "    transcripts_sub['translate_y'] = transcripts_sub.mosaic_y - raw_y_start\n",
    "    \n",
    "    point_of_interest = Point(x_ax, y_ax)\n",
    "    \n",
    "    fig, ax = plt.subplots()\n",
    "    ax.imshow(sub_image, cmap='gray', extent=(x_start, x_end, y_start, y_end))\n",
    "    \n",
    "    for geometry in clipped_geometries:\n",
    "        if isinstance(geometry, Polygon):\n",
    "            # Highlight in blue if the geometry contains the point\n",
    "            color = 'blue' if geometry.contains(point_of_interest) else 'red'\n",
    "            xs, ys = geometry.exterior.xy\n",
    "            ax.plot(xs, ys, color=color)\n",
    "        elif isinstance(geometry, MultiPolygon):\n",
    "            for poly in geometry.geoms:\n",
    "                # Highlight in blue if any polygon in MultiPolygon contains the point\n",
    "                color = 'blue' if poly.contains(point_of_interest) else 'red'\n",
    "                xs, ys = poly.exterior.xy\n",
    "                ax.plot(xs, ys, color=color)\n",
    "    if save_path:\n",
    "        plt.savefig(save_path,format='pdf')\n",
    "    plt.show()\n",
    "    \n",
    "    return sub_image, sub_dapi, transcripts_sub\n",
    "\n",
    "def segment_image(im,window_size,foreground=True, dapi=False):\n",
    "    if im.dtype == 'uint16':\n",
    "        im = ((im - im.min()) / (im.max() - im.min()) * 255).astype(np.uint8)\n",
    "    subtract = cv2.fastNlMeansDenoising(im)\n",
    "    if foreground:\n",
    "        pre = cv2.adaptiveThreshold((255 - subtract), 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C,\n",
    "                                cv2.THRESH_BINARY, window_size, 2)\n",
    "    else:\n",
    "        pre = cv2.adaptiveThreshold((subtract), 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C,\n",
    "                                cv2.THRESH_BINARY, window_size, 2)\n",
    "    opened = opening(255 - pre, disk(3))\n",
    "    pre = closing(opened, disk(3))\n",
    "    filled_image = binary_fill_holes(pre).astype(np.uint8)\n",
    "        \n",
    "    return filled_image\n",
    "\n",
    "def roi_picker(im, point=(500, 500), dapi=False):\n",
    "    labeled_array, num_features = label(im)\n",
    "    \n",
    "    if dapi:\n",
    "        # Apply distance transform\n",
    "        distance = distance_transform_edt(labeled_array > 0)\n",
    "        \n",
    "        # Generate markers using connected components after thresholding\n",
    "        coords = peak_local_max(distance, footprint=np.ones((9, 9)), labels=(labeled_array > 0))\n",
    "        mask = np.zeros(distance.shape, dtype=bool)\n",
    "        mask[tuple(coords.T)] = True\n",
    "        markers, _ = label(mask)\n",
    "        # Apply watershed\n",
    "        watershed_labels = watershed(-distance, markers, mask=(labeled_array > 0))\n",
    "        \n",
    "        # Update the labeled array with the watershed labels\n",
    "        labeled_array = watershed_labels\n",
    "\n",
    "        # Initialize variables to track the closest region\n",
    "        closest_region_label = None\n",
    "        min_distance = float('inf')\n",
    "        regions = regionprops(labeled_array)\n",
    "\n",
    "        # Find the closest region to the specified point\n",
    "        for region in regions:\n",
    "            if region.area > 500:\n",
    "                # Calculate the distance from the point to the region's centroid\n",
    "                region_centroid = np.array(region.centroid)\n",
    "                distance = np.linalg.norm(region_centroid - np.array(point))\n",
    "\n",
    "                # Update if this region is closer than previous ones\n",
    "                if distance < min_distance:\n",
    "                    min_distance = distance\n",
    "                    closest_region_label = region.label\n",
    "\n",
    "        if closest_region_label is not None:\n",
    "            isolated_component = labeled_array == closest_region_label\n",
    "            label_image = np.zeros_like(labeled_array)\n",
    "            label_image[isolated_component] = 1\n",
    "            return label_image\n",
    "        else:\n",
    "            return labeled_array\n",
    "    else:\n",
    "        # The original functionality to find the largest region near the point\n",
    "        largest_component_label = None\n",
    "        max_area = 0\n",
    "        regions = regionprops(labeled_array)\n",
    "        for region in regions:\n",
    "            if region.area > 500:\n",
    "                boundary_coords = np.column_stack(np.where(labeled_array == region.label))\n",
    "                distances = np.linalg.norm(boundary_coords - np.array(point), axis=1)\n",
    "                min_dist = np.min(distances)\n",
    "\n",
    "                if min_dist < 50:\n",
    "                    if region.area > max_area:\n",
    "                        max_area = region.area\n",
    "                        largest_component_label = region.label\n",
    "\n",
    "        if largest_component_label is not None:\n",
    "            isolated_component = labeled_array == largest_component_label\n",
    "            label_image = np.zeros_like(labeled_array)\n",
    "            label_image[isolated_component] = 1\n",
    "            return label_image\n",
    "        else:\n",
    "            return labeled_array\n",
    "        \n",
    "def count_gene_overlaps(transcripts, dapi, micro, filled_dapi):\n",
    "    \"\"\"\n",
    "    Counts the occurrences of barcodes (rows) for each gene that overlap with dapi_1, \n",
    "    with the binary difference of micro_1 - dapi_1, and with the binary difference of micro_1 - filled_dapi.\n",
    "    \n",
    "    Parameters:\n",
    "        transcripts (pd.DataFrame): A DataFrame containing 'genes', 'translate_x', and 'translate_y' columns.\n",
    "        dapi_1 (np.array): Binary image representing the region of interest (e.g., DAPI stained area).\n",
    "        micro_1 (np.array): Binary image representing a larger or different region of interest.\n",
    "        filled_dapi (np.array): Binary image representing another region of interest.\n",
    "        \n",
    "    Returns:\n",
    "        result (pd.DataFrame): A subset of the input DataFrame 'transcripts' containing only the barcodes that overlap with\n",
    "                               the binary difference between micro_1 and dapi_1 or the binary difference between \n",
    "                               micro_1 and filled_dapi, along with overlap counts per gene.\n",
    "        dapi_only (pd.DataFrame): A subset of the input DataFrame 'transcripts' containing only the barcodes that overlap exclusively with dapi_1.\n",
    "    \"\"\"\n",
    "    \n",
    "    transcripts = transcripts.copy()\n",
    "    # Calculate the differences\n",
    "    binary_diff_dapi = np.logical_and(micro.astype(bool), np.logical_not(dapi.astype(bool)))\n",
    "    binary_diff_filled_dapi = np.logical_and(micro.astype(bool), np.logical_not(filled_dapi.astype(bool)))\n",
    "\n",
    "    # Initialize counts for overlap\n",
    "    #transcripts['overlap_dapi'] = 0\n",
    "    #transcripts['overlap_diff_filled_dapi'] = 0\n",
    "\n",
    "    # Iterate over the transcripts and check for overlap\n",
    "    #for index, row in transcripts.iterrows():\n",
    "     #   x, y = int(row['translate_x']), int(row['translate_y'])\n",
    "     #   if dapi[y, x]:  # Check if the point overlaps with dapi_1\n",
    "     #       transcripts.at[index, 'overlap_dapi'] = 1\n",
    "     #   if binary_diff_filled_dapi[y, x]:  # Check if the point overlaps with the binary difference micro_1 - filled_dapi\n",
    "     #       transcripts.at[index, 'overlap_diff_filled_dapi'] = 1\n",
    "\n",
    "    # Subset the DataFrame to only include rows where there is an overlap in the desired regions\n",
    "    #result = transcripts[\n",
    "     #   (transcripts['overlap_diff_filled_dapi'] > 0) &\n",
    "      #  (transcripts['overlap_dapi'] == 0)\n",
    "    #]\n",
    "\n",
    "    # Group by gene and sum the overlaps\n",
    "\n",
    "    # Subset the DataFrame to only include rows where there is overlap exclusively with dapi_1\n",
    "    #dapi_only = transcripts[\n",
    "     #   (transcripts['overlap_dapi'] > 0) &\n",
    "     #   (transcripts['overlap_diff_filled_dapi'] == 0)\n",
    "    #]\n",
    "    \n",
    "    results = transcripts[\n",
    "        (transcripts['translate_x'].astype(int) >= 0) & (transcripts['translate_x'].astype(int) < binary_diff_filled_dapi.shape[1]) &\n",
    "        (transcripts['translate_y'].astype(int) >= 0) & (transcripts['translate_y'].astype(int) < binary_diff_filled_dapi.shape[0])\n",
    "    ]\n",
    "\n",
    "    results = results[\n",
    "        binary_diff_filled_dapi[results['translate_y'].astype(int), results['translate_x'].astype(int)]\n",
    "    ]\n",
    "    \n",
    "    dapi_only = transcripts[\n",
    "        (transcripts['translate_x'].astype(int) >= 0) & (transcripts['translate_x'].astype(int) < dapi.shape[1]) &\n",
    "        (transcripts['translate_y'].astype(int) >= 0) & (transcripts['translate_y'].astype(int) < dapi.shape[0])\n",
    "    ]\n",
    "\n",
    "    dapi_only = dapi_only[\n",
    "        dapi.astype(bool)[dapi_only['translate_y'].astype(int), dapi_only['translate_x'].astype(int)]\n",
    "    ]\n",
    "    \n",
    "\n",
    "    return results, dapi_only\n",
    "\n",
    "def calculate_areas(dataframe, dapi_labeled_array, non_dapi_labeled_array):\n",
    "    \"\"\"\n",
    "    Calculate the total area of the DAPI (nucleus) and non-DAPI (non-nucleus) regions.\n",
    "\n",
    "    Parameters:\n",
    "        dataframe (pd.DataFrame): A DataFrame with one row, which will be used to create the output DataFrame.\n",
    "        dapi_labeled_array (np.array): A labeled array where each unique integer represents a different object in the nucleus (DAPI).\n",
    "        non_dapi_labeled_array (np.array): A labeled array where each unique integer represents a different object in the non-nucleus (Non-DAPI).\n",
    "\n",
    "    Returns:\n",
    "        result_df (pd.DataFrame): A DataFrame with the same index as the input DataFrame, containing two columns:\n",
    "                                  'DAPI Area' and 'Non-DAPI Area'.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Calculate total area for DAPI (nucleus)\n",
    "    dapi_total_area = sum(region.area for region in regionprops(dapi_labeled_array))\n",
    "    \n",
    "    # Calculate total area for Non-DAPI (non-nucleus)\n",
    "    non_dapi_total_area = sum(region.area for region in regionprops(non_dapi_labeled_array))\n",
    "    \n",
    "    # Create a new DataFrame to store the results\n",
    "    result_df = pd.DataFrame({\n",
    "        'DAPI Area': [dapi_total_area],\n",
    "        'Non-DAPI Area': [non_dapi_total_area]\n",
    "    }, index=dataframe.index)\n",
    "    \n",
    "    return result_df\n",
    "\n",
    "def generate_counts_matrix(dataframe, var_names):\n",
    "    \"\"\"\n",
    "    Generate a counts matrix where columns are genes and rows contain the number of barcodes \n",
    "    for that gene present in the dataframe.\n",
    "\n",
    "    Parameters:\n",
    "        dataframe (pd.DataFrame): The input DataFrame containing barcode information.\n",
    "        var_names (list or pd.Index): List of gene names (matching adata.var_names).\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A DataFrame with genes as columns and the number of barcodes for each gene.\n",
    "    \"\"\"\n",
    "    # Filter the dataframe to only include genes in var_names\n",
    "    filtered_df = dataframe[dataframe['gene'].isin(var_names)]\n",
    "    \n",
    "    # Count the number of barcodes for each gene\n",
    "    counts = filtered_df['gene'].value_counts().reindex(var_names, fill_value=0)\n",
    "    \n",
    "    # Convert the counts to a DataFrame\n",
    "    counts_df = counts.to_frame().T\n",
    "    \n",
    "    return counts_df\n",
    "\n",
    "def rename_index(df, adata,blanks):\n",
    "    counts_matrix_result = generate_counts_matrix(df, adata.var_names.tolist()+blanks)\n",
    "    counts_matrix_result.index = adata.obs.index\n",
    "    return counts_matrix_result\n",
    "\n",
    "def generate_transcript_spreadsheet(transcripts, dapi, micro, ad_test):\n",
    "    # Calculate the union of dapi_1 and micro_1\n",
    "    union_mask = np.logical_or(dapi.astype(bool), micro.astype(bool))\n",
    "    \n",
    "    #transcripts['overlap'] = 0\n",
    "    \n",
    "    #for index, row in transcripts.iterrows():\n",
    "     #   x, y = int(row['translate_x']), int(row['translate_y'])\n",
    "     #   if union_mask[y, x]:\n",
    "      #      transcripts.at[index, 'overlap'] = 1\n",
    "            \n",
    "    #filtered_transcripts = transcripts[transcripts.overlap > 0]\n",
    "    \n",
    "    \n",
    "\n",
    "    # Filter transcripts to include only those within the union of dapi_1 and micro_1\n",
    "    filtered_transcripts = transcripts[\n",
    "        (transcripts['translate_x'].astype(int) >= 0) & (transcripts['translate_x'].astype(int) < union_mask.shape[1]) &\n",
    "        (transcripts['translate_y'].astype(int) >= 0) & (transcripts['translate_y'].astype(int) < union_mask.shape[0])\n",
    "    ]\n",
    "\n",
    "    filtered_transcripts = filtered_transcripts[\n",
    "        union_mask[filtered_transcripts['translate_y'].astype(int), filtered_transcripts['translate_x'].astype(int)]\n",
    "    ]\n",
    "\n",
    "    # Create a new DataFrame for the spreadsheet\n",
    "    spreadsheet_df = pd.DataFrame({\n",
    "        'x': filtered_transcripts['translate_x'].astype(int),\n",
    "        'y': filtered_transcripts['translate_y'].astype(int),\n",
    "        'gene': filtered_transcripts['gene'],\n",
    "        'cell': ad_test.obs.Name.iloc[0]\n",
    "    })\n",
    "    return spreadsheet_df\n",
    "\n",
    "def add_scale_bar(ax, conversion_rate, micron_length, color='red', location=(10, 10), thickness=50, fontsize=12):\n",
    "    \"\"\"\n",
    "    Add a scale bar to the given axis.\n",
    "\n",
    "    Args:\n",
    "    - ax: The axis on which to draw the scale bar.\n",
    "    - conversion_rate: The pixel-to-micron conversion factor (pixels per micron).\n",
    "    - micron_length: The length of the scale bar in microns.\n",
    "    - color: The color of the scale bar.\n",
    "    - location: A tuple specifying the (x, y) location of the scale bar.\n",
    "    - thickness: The thickness of the scale bar in pixels.\n",
    "    \"\"\"\n",
    "    # Calculate the length of the scale bar in pixels\n",
    "    pixel_length = conversion_rate * micron_length\n",
    "    \n",
    "    # Create a rectangle for the scale bar\n",
    "    scale_bar = mpatches.Rectangle(location, pixel_length, thickness, linewidth=0, edgecolor=color, facecolor=color)\n",
    "    \n",
    "    # Add the scale bar to the plot\n",
    "    ax.add_patch(scale_bar)\n",
    "    text_x = location[0] + pixel_length + 10  # Place text to the right of the scale bar\n",
    "    text_y = location[1] + thickness / 2      # Vertically center the text with the scale bar\n",
    "    ax.text(text_x, text_y, f'{micron_length} Î¼m', color=color, fontsize=fontsize, va='center')\n",
    "    \n",
    "mosaic_to_micron = pd.read_csv('/hpc/projects/group.quake/doug/Shapes_Spatial/3-mo-female-1-rev2/images/micron_to_mosaic_pixel_transform.csv',delimiter=' ').iloc[0,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0801578e-6984-4716-b81c-9ce5649faaa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "ad_parent = sc.read_h5ad('Shape_500.h5ad')\n",
    "ad_parent = ad_parent[ad_parent.obs.updated_celltype == 'Microglia']\n",
    "experiment = '/hpc/projects/group.quake/doug/Shapes_Spatial/24-mo-female-1/'\n",
    "\n",
    "batch = experiment.split('/')[-2]\n",
    "    # We also want to load in the geometry file\n",
    "ad_viz = ad_parent[ad_parent.obs.batchID == batch]\n",
    "transform_file = f'{experiment}/images/micron_to_mosaic_pixel_transform.csv'\n",
    "transform_matrix = pd.read_table(transform_file, sep=' ', header=None).iloc[:2]\n",
    "transcripts = find_filtered_transcripts(experiment)\n",
    "transcripts['mosaic_x'], transcripts['mosaic_y'] = transcripts.global_x * transform_matrix.iloc[0,0] + transform_matrix.iloc[0,2], transcripts.global_y * transform_matrix.iloc[1,1] + transform_matrix.iloc[1,2]\n",
    "blank_names = transcripts[transcripts.gene.str.contains('Blank')].gene.unique().tolist()\n",
    "\n",
    "root = '/hpc/projects/group.quake/doug/Shapes_Spatial/'\n",
    "raw_im = Mapping.load_tiff_image(root + batch + '/binary_image.tif')\n",
    "raw_dapi = Mapping.load_tiff_image(root + batch + '/images/mosaic_DAPI_z4.tif')\n",
    "raw_polyT = Mapping.load_tiff_image(root + batch + '/images/mosaic_PolyT_z4.tif')\n",
    "final_df_non_nuc = pd.DataFrame(columns=ad_viz.var_names.tolist()+blank_names)\n",
    "final_df_nuc = pd.DataFrame(columns=ad_viz.var_names.tolist()+blank_names)\n",
    "area_df = pd.DataFrame(columns=['DAPI Area','Non-DAPI Area'])\n",
    "boundaries = gpd.read_parquet(f\"{experiment}baysor/region_0_6-5_micron_polygons.parquet\")\n",
    "geometries = boundaries['Geometry']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d569329-88d4-499d-a092-fd65d7a6022a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ad_test = ad_viz[ad_viz.obs.morph_leiden == 4][4,:]\n",
    "small_raw, small_dapi, small_transcripts = load_images(batch, ad_test.obs.x.iloc[0], ad_test.obs.y.iloc[0],raw_im, raw_dapi,transcripts,geometries,save_path='supp_figures/SI_2/mic_1_baysor.pdf')\n",
    "filled_raw = segment_image(small_raw, 115, foreground=True)\n",
    "filled_dapi = segment_image(small_dapi, 255, foreground=True, dapi=True)\n",
    "    \n",
    "micro_1 = roi_picker(filled_raw)\n",
    "dapi_1 = roi_picker(filled_dapi,dapi=True)\n",
    "    \n",
    "counts_non_nuclei, counts_nuclei = count_gene_overlaps(small_transcripts,dapi_1,micro_1,filled_dapi)\n",
    "plt.imshow(np.logical_or(dapi_1.astype(bool), micro_1.astype(bool)),cmap='gray')\n",
    "\n",
    "total_counts = pd.concat([counts_nuclei,counts_non_nuclei])\n",
    "\n",
    "unique_genes = total_counts['gene'].unique()\n",
    "color_map = sns.color_palette('hsv', len(unique_genes))  # You can use other color palettes if preferred\n",
    "gene_to_color = {gene: color_map[i] for i, gene in enumerate(unique_genes)}\n",
    "\n",
    "# Get the colors for each gene in the DataFrame\n",
    "colors = total_counts['gene'].map(gene_to_color)\n",
    "\n",
    "# Plot the background (dapi and micro images)\n",
    "plt.imshow(np.logical_or(dapi_1.astype(bool), micro_1.astype(bool)), cmap='gray')\n",
    "\n",
    "# Scatter plot with different colors for each gene\n",
    "plt.scatter(total_counts['translate_x'], total_counts['translate_y'], c=colors)\n",
    "\n",
    "# Add a legend (optional)\n",
    "handles = [plt.Line2D([0], [0], marker='o', color=color_map[i], linestyle='', markersize=10, label=gene) \n",
    "           for i, gene in enumerate(unique_genes)]\n",
    "plt.legend(handles=handles, bbox_to_anchor=(1.05, 1), loc='upper left', borderaxespad=0.)\n",
    "\n",
    "plt.savefig('supp_figures/SI_2/mic_1_transcripts.pdf',format='pdf')\n",
    "# Display the plot\n",
    "plt.show()\n",
    "\n",
    "fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(5, 5))\n",
    "\n",
    "ax.imshow(small_raw,cmap='gray')\n",
    "add_scale_bar(ax, mosaic_to_micron, 50, color='red', location=(10, 10), thickness=50, fontsize=12)\n",
    "plt.savefig('supp_figures/SI_2/mic_1_raw.pdf',format='pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39a07ed7-c5a3-4b08-929b-44cecee8c647",
   "metadata": {},
   "source": [
    "# Supplemental Figure 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71aac4f3-96d2-4f64-aa95-0e9e310e5e44",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scanpy as sc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import gseapy as gp\n",
    "from tqdm import tqdm\n",
    "from adjustText import adjust_text\n",
    "import seaborn as sns\n",
    "import anndata\n",
    "\n",
    "from sklearn.neighbors import BallTree, KDTree\n",
    "from sklearn.neighbors import KNeighborsClassifier, KNeighborsRegressor\n",
    "from scipy.sparse import csr_matrix, vstack\n",
    "from tqdm import tqdm\n",
    "from joblib import delayed, Parallel\n",
    "import seaborn as sns\n",
    "import os\n",
    "\n",
    "from scipy.stats import sem, wilcoxon\n",
    "from itertools import combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2da3a068-095d-45d0-a849-75fc7b5797d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_spatial_with_highlight(adata, cell_type_column, cell_type_to_highlight, spatial_key='spatial',save_path=None):\n",
    "    \"\"\"\n",
    "    Plot spatial coordinates of the whole brain, highlighting only a specific cell type.\n",
    "    \n",
    "    Parameters:\n",
    "    - adata: AnnData object with spatial data in `.obsm[spatial_key]`.\n",
    "    - cell_type_column: Column in `adata.obs` containing cell type annotations.\n",
    "    - cell_type_to_highlight: The specific cell type to highlight in the plot.\n",
    "    - spatial_key: The key in `.obsm` for spatial coordinates (default is 'spatial').\n",
    "    \"\"\"\n",
    "    # Extract spatial coordinates\n",
    "    spatial_coords = adata.obsm[spatial_key]\n",
    "\n",
    "    # Create a mask for the cells to highlight\n",
    "    highlight_mask = adata.obs[cell_type_column] == cell_type_to_highlight\n",
    "\n",
    "    # Plot all cells in grey\n",
    "    plt.figure(figsize=(20, 8))\n",
    "    plt.scatter(spatial_coords[:, 0], spatial_coords[:, 1], c='lightgrey', s=5, label='Other cells')\n",
    "\n",
    "    # Overlay the highlighted cells in a different color (e.g., red)\n",
    "    plt.scatter(spatial_coords[highlight_mask, 0], spatial_coords[highlight_mask, 1], c='red', s=5, label=cell_type_to_highlight)\n",
    "\n",
    "    # Add titles and labels\n",
    "    plt.title(f\"Spatial plot highlighting {cell_type_to_highlight}\")\n",
    "    plt.xlabel('Spatial X')\n",
    "    plt.ylabel('Spatial Y')\n",
    "    plt.legend()\n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=300)\n",
    "\n",
    "    #plt.gca().invert_yaxis()  # Invert y-axis to match spatial conventions\n",
    "    plt.show()\n",
    "\n",
    "def plot_stacked_bar_by_age(adata, age_column='Age', sub_mic_column='sub_mic',output_pdf=None):\n",
    "    \"\"\"\n",
    "    Plot a stacked bar plot showing the percentage of each sub_mic category at each age.\n",
    "    \n",
    "    Parameters:\n",
    "    - adata: AnnData object with `Age` and `sub_mic` information in `.obs`.\n",
    "    - age_column: The column in `adata.obs` that represents the age (default is 'Age').\n",
    "    - sub_mic_column: The column in `adata.obs` that represents the sub_mic categories (default is 'sub_mic').\n",
    "    \n",
    "    Returns:\n",
    "    - A stacked bar plot showing the percentage of each sub_mic category at each age.\n",
    "    \"\"\"\n",
    "    # Create a DataFrame with Age and sub_mic information\n",
    "    df = adata.obs[[age_column, sub_mic_column]]\n",
    "    \n",
    "    # Group by Age and sub_mic and count occurrences\n",
    "    grouped = df.groupby([age_column, sub_mic_column]).size().unstack(fill_value=0)\n",
    "    \n",
    "    # Calculate the percentage for each sub_mic category by age\n",
    "    percentage_df = grouped.div(grouped.sum(axis=1), axis=0) * 100\n",
    "\n",
    "    # Plot the stacked bar plot\n",
    "    percentage_df.plot(kind='bar', stacked=True, figsize=(6, 6), cmap='tab20')\n",
    "\n",
    "    # Customize the plot\n",
    "    plt.title('Percentage of each sub_mic by Age')\n",
    "    plt.xlabel('Age')\n",
    "    plt.ylabel('Percentage')\n",
    "    plt.legend(title=sub_mic_column, bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    plt.tight_layout()\n",
    "    if output_pdf:\n",
    "        plt.savefig(output_pdf,format='pdf')\n",
    "\n",
    "    # Show the plot\n",
    "    plt.show()  \n",
    "\n",
    "def plot_and_export_density_with_stats(adata, adata_total, sub_mic_value, region_column='Brain_Region', age_column='Age', batch_column='batchID', sub_mic_column='sub_mic', output_pdf=None):\n",
    "    \"\"\"\n",
    "    Plot density of a specific sub_mic category and perform pairwise Wilcoxon signed-rank tests between region-age pairs.\n",
    "    \n",
    "    Parameters:\n",
    "    - adata: AnnData object with subset of cells for the sub_mic category.\n",
    "    - adata_total: AnnData object with all cells for density calculations.\n",
    "    - sub_mic_value: Specific sub_mic category to analyze.\n",
    "    - region_column, age_column, batch_column, sub_mic_column: Column names in `.obs`.\n",
    "    - output_pdf: Path to save plot as a PDF.\n",
    "    \n",
    "    Returns:\n",
    "    - stats_df: DataFrame with density values for plotting.\n",
    "    - wilcoxon_df: DataFrame with pairwise Wilcoxon signed-rank test results.\n",
    "    \"\"\"\n",
    "    # Filter for cells matching sub_mic_value and get counts by region, age, and batch\n",
    "    df = adata.obs[[region_column, age_column, batch_column, sub_mic_column]]\n",
    "    sub_mic_df = df[df[sub_mic_column] == sub_mic_value]\n",
    "    sub_mic_counts_by_region = sub_mic_df.groupby([region_column, age_column, batch_column]).size().reset_index(name='sub_mic_counts')\n",
    "    \n",
    "    # Total cell counts from adata_total for each region, age, and batch\n",
    "    total_df = adata_total.obs[[region_column, age_column, batch_column]]\n",
    "    total_counts_by_region = total_df.groupby([region_column, age_column, batch_column]).size().reset_index(name='total_counts')\n",
    "\n",
    "    # Merge sub_mic counts with total counts and calculate density\n",
    "    merged_df = pd.merge(sub_mic_counts_by_region, total_counts_by_region, on=[region_column, age_column, batch_column], how='left')\n",
    "    merged_df['Density'] = merged_df['sub_mic_counts'] / merged_df['total_counts']\n",
    "\n",
    "    # Calculate mean and SEM for each region and age\n",
    "    stats_df = merged_df.groupby([region_column, age_column]).agg(\n",
    "        mean_density=('Density', 'mean'),\n",
    "        sem_density=('Density', sem)\n",
    "    ).reset_index()\n",
    "\n",
    "    # Plot density by region and age\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.barplot(data=merged_df, x=region_column, y='Density', hue=age_column, palette='Set2', capsize=0.1, errorbar='se')\n",
    "    sns.stripplot(data=merged_df, x=region_column, y='Density', hue=age_column, dodge=True, jitter=True, marker='o', palette='dark', size=5, alpha=0.7)\n",
    "    plt.title(f'Density of {sub_mic_value} in Each Region by Age (Relative to Total Cell Count)')\n",
    "    plt.xlabel('Region')\n",
    "    plt.ylabel('Density of Cell Type')\n",
    "    plt.legend(title='Age')\n",
    "    plt.xticks(rotation=90, ha='right')\n",
    "    plt.tight_layout()\n",
    "    if output_pdf:\n",
    "        plt.savefig(output_pdf, format='pdf')\n",
    "    plt.show()\n",
    "\n",
    "    # Perform pairwise Wilcoxon signed-rank tests between each pair of region-age groups\n",
    "    pairwise_results = []\n",
    "    merged_df['Density'] = merged_df['Density'].fillna(0)\n",
    "    unique_pairs = list(combinations(merged_df[[region_column, age_column]].drop_duplicates().itertuples(index=False, name=None), 2))\n",
    "\n",
    "    for (region1, age1), (region2, age2) in unique_pairs:\n",
    "        # Extract density values for the two groups\n",
    "        density_group1 = merged_df[(merged_df[region_column] == region1) & (merged_df[age_column] == age1)]['Density']\n",
    "        density_group2 = merged_df[(merged_df[region_column] == region2) & (merged_df[age_column] == age2)]['Density']\n",
    "        \n",
    "        # Ensure they have matching batch counts for paired testing\n",
    "        shared_batches = set(merged_df[merged_df[region_column] == region1][batch_column]) & set(merged_df[merged_df[region_column] == region2][batch_column])\n",
    "        density_group1 = density_group1[merged_df[batch_column].isin(shared_batches)]\n",
    "        density_group2 = density_group2[merged_df[batch_column].isin(shared_batches)]\n",
    "        \n",
    "        # Only conduct test if both groups have values and at least two shared batches\n",
    "        if len(density_group1) > 1 and len(density_group2) > 1:\n",
    "            stat, p_value = wilcoxon(density_group1, density_group2)\n",
    "            pairwise_results.append({\n",
    "                'Region_Age_1': f\"{region1}_{age1}\",\n",
    "                'Region_Age_2': f\"{region2}_{age2}\",\n",
    "                'Wilcoxon_statistic': stat,\n",
    "                'p_value': p_value\n",
    "            })\n",
    "\n",
    "    # Convert results to DataFrame\n",
    "    wilcoxon_df = pd.DataFrame(pairwise_results)\n",
    "    return stats_df, wilcoxon_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "245a73cd-336c-48c0-abcf-fc40463015c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "adata = sc.read_h5ad('ABC_cleaned.h5ad')\n",
    "new_ad = sc.read_h5ad('Transciptomic_labels_and_morphology_labels_full.h5ad')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3107c56-54e0-42a1-98a2-683219defe2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_spatial_with_highlight(adata[adata.obs.batchID=='3-mo-male-3-rev2'], cell_type_column='updated_celltype', cell_type_to_highlight='Microglia',save_path='young_microglia_on_brain.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e2b88ae-da8b-4938-a38d-fc2466d1491c",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_ad = new_ad[new_ad.obs.Brain_Region != 'Unlabeled']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82a544b8-a958-4ae7-93aa-e1fa2380196e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_stacked_bar_by_age(new_ad,output_pdf='stacked_bar.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c93bff2b-d361-44d2-9e30-a8f60d8b371e",
   "metadata": {},
   "outputs": [],
   "source": [
    "_,pvals = plot_and_export_density_with_stats(new_ad, total, 'DAM', region_column='Brain_Region', age_column='Age', batch_column='batchID', sub_mic_column='sub_mic', output_pdf='DAM_Microglia_Density.pdf')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c95ed50d-3a48-4827-b488-f5e13009fbb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "_,pvals = plot_and_export_density_with_stats(new_ad, total, 'Homeostatic', region_column='Brain_Region', age_column='Age', batch_column='batchID', sub_mic_column='sub_mic', output_pdf='Homeostatic_Microglia_Density.pdf')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "477e2d14-693b-418c-afce-a77ef7239df4",
   "metadata": {},
   "outputs": [],
   "source": [
    "_,pvals = plot_and_export_density_with_stats(new_ad, total, 'Transitioning', region_column='Brain_Region', age_column='Age', batch_column='batchID', sub_mic_column='sub_mic', output_pdf='Transitioning_Microglia_Density.pdf')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe8df9fd-5e6d-41f0-844c-965d450659c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.pl.umap(new_ad,color='Brain_Region',save='Region_umap.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "289a6f60-31f3-4923-bc41-acf47cabf7d9",
   "metadata": {},
   "source": [
    "# Supplemental Figure 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "061503ff-7a8e-4d55-aac2-3a909e12f209",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import scipy.stats as stats\n",
    "import pandas as pd\n",
    "from statsmodels.stats.multitest import multipletests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d97c97a-e254-48a6-bc2b-471c96dbdf31",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_ad = sc.read_h5ad('Transciptomic_labels_and_morphology_labels_full.h5ad')\n",
    "\n",
    "start_index = list(new_ad.obs.columns).index('Cell Area')  # Add 1 to include the column itself\n",
    "columns_after_specific = new_ad.obs.iloc[:, start_index:].values\n",
    "\n",
    "morphological_columns = new_ad.obs.columns[start_index:]\n",
    "\n",
    "features = morphological_columns[:-11].tolist()\n",
    "\n",
    "features_set = set(features) - set(['Radius of Influence'])\n",
    "features = [i for i in features_set]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "555e3175-92b2-4766-9800-9e81caad5266",
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_order = ['Transitioning', 'Homeostatic', 'DAM']\n",
    "\n",
    "fig, axes = plt.subplots(5, 6, figsize=(25, 25))  # Adjust grid size as needed\n",
    "anova_results = {}  # Dictionary to store ANOVA results for each feature\n",
    "\n",
    "# Collect all p-values to adjust them after loop\n",
    "all_p_values = []\n",
    "\n",
    "for i, feature in enumerate(features):\n",
    "    row = i // 6\n",
    "    col = i % 6\n",
    "    y_feature = feature  # The y-axis feature to plot\n",
    "\n",
    "    # Apply the custom order to the 'sub_mic' column in the violin plot\n",
    "    sns.violinplot(x='sub_mic', y=y_feature, data=new_ad.obs, ax=axes[row, col], inner='box', order=custom_order)\n",
    "    \n",
    "    # Perform ANOVA\n",
    "    groups = [new_ad.obs[new_ad.obs['sub_mic'] == cluster][y_feature].dropna() for cluster in custom_order]\n",
    "    f_stat, p_value = stats.f_oneway(*groups)  # ANOVA calculation\n",
    "    \n",
    "    # Store the initial result and save the p-value for multiple testing correction\n",
    "    anova_results[feature] = {'F-statistic': f_stat, 'p-value': p_value}\n",
    "    all_p_values.append(p_value)\n",
    "\n",
    "# Apply multiple testing correction to all p-values\n",
    "_, corrected_p_values, _, _ = multipletests(all_p_values, method='fdr_bh')  # Benjamini-Hochberg (FDR) correction\n",
    "\n",
    "# Update anova_results with corrected p-values\n",
    "for i, feature in enumerate(features):\n",
    "    anova_results[feature]['corrected p-value'] = corrected_p_values[i]\n",
    "\n",
    "    # Update plot titles to include corrected p-value\n",
    "    row = i // 6\n",
    "    col = i % 6\n",
    "    if i < 25:  # To ensure we don't exceed subplot limits\n",
    "        axes[row, col].set_title(f\"{feature.replace('_', ' ').title()} (p={corrected_p_values[i]:.3f})\")\n",
    "\n",
    "# Save the plot\n",
    "plt.tight_layout()\n",
    "plt.savefig('figures/morphology_by_cluster_custom_order_rotated_labels.pdf', format='pdf', bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Optionally, display or save the ANOVA results\n",
    "anova_df = pd.DataFrame(anova_results).T  # Convert to DataFrame for easy viewing\n",
    "print(anova_df) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Vizgen_2",
   "language": "python",
   "name": "vizgen_2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
