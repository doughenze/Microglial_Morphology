{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b91e8fa9-dd7c-494b-8c2e-9de16d20d2d1",
   "metadata": {},
   "source": [
    "The code to analyze the permutations is adapted from: https://github.com/ZhuangLab/whole_mouse_brain_MERFISH_atlas_scripts_2023/blob/main/scripts/cell_cell_contacts/get_significant_contacts_30um.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffa878ac-995c-4d22-b032-e27a60785b4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scanpy as sc\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import os\n",
    "import string\n",
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "from shapely.geometry import Point\n",
    "from tqdm import tqdm\n",
    "import Mapping\n",
    "import matplotlib.pyplot as plt\n",
    "import multiprocessing as mp\n",
    "from functools import partial\n",
    "import gseapy as gp\n",
    "import networkx as nx\n",
    "\n",
    "import skimage\n",
    "import cv2\n",
    "from skimage.morphology import disk, opening, closing\n",
    "from scipy.ndimage import binary_fill_holes, label, distance_transform_edt\n",
    "from skimage.segmentation import find_boundaries, watershed\n",
    "from skimage.feature import peak_local_max\n",
    "from skimage.measure import regionprops\n",
    "from skimage.segmentation import watershed\n",
    "import scipy.stats\n",
    "import statsmodels.stats.multitest\n",
    "\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from node2vec import Node2Vec\n",
    "import networkx as nx\n",
    "from matplotlib.lines import Line2D"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5801f244-b50f-4ae7-b1d0-2ab8f6cc8f89",
   "metadata": {},
   "source": [
    "# Clustering Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a33cc062-7659-43b5-bd24-1db29708f1ff",
   "metadata": {},
   "source": [
    "This section was directly adapted from the DypFISH repo: https://github.com/cbib/dypfish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f9af617-8821-4b0f-a8b5-14095c0fc7ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_filtered_transcripts(experiment_path):\n",
    "    region_types = ['region_0', 'region_1']\n",
    "    for region in region_types:\n",
    "        file_path = f'{experiment_path}baysor/detected_transcripts.csv'\n",
    "        if os.path.exists(file_path):\n",
    "            return pd.read_csv(file_path,index_col=0)\n",
    "    return None\n",
    "\n",
    "def find_filtered_transcripts(experiment_path):\n",
    "    region_types = ['region_0', 'region_1']\n",
    "    for region in region_types:\n",
    "        file_path = f'{experiment_path}baysor/detected_transcripts.csv'\n",
    "        if os.path.exists(file_path):\n",
    "            return pd.read_csv(file_path,index_col=0)\n",
    "    return None\n",
    "\n",
    "def sem(a: np.array, factor=3) -> float:\n",
    "    \"\"\"\n",
    "    sem in presence of extreme outliers (very skewed distribution), factor = 0 gives standard behaviour\n",
    "    \"\"\"\n",
    "    if factor > 0:\n",
    "        limit = factor * np.std(a)\n",
    "        a = a[(a < np.mean(a) + limit) & (a > np.mean(a) - limit)]\n",
    "    return scipy.stats.sem(a, ddof=0)\n",
    "\n",
    "def mean_confidence_interval(a: np.array, cutoff=.95):\n",
    "    ''' cutoff is the significance level as a decimal between 0 and 1'''\n",
    "    a = np.sort(a)\n",
    "    factor = scipy.stats.norm.ppf((1 + cutoff) / 2)\n",
    "    factor *= math.sqrt(len(a))  # avoid doing computation twice\n",
    "\n",
    "    lix = int(0.5 * (len(a) - factor)) + 1\n",
    "    uix = int(0.5 * (1 + len(a) + factor)) + 1\n",
    "    if lix < 0:\n",
    "        lix = 0\n",
    "    if uix > len(a)-1:\n",
    "        uix = len(a)-1\n",
    "    assert (lix > 0), \"index \" + str(lix) + \" is out of bound for array of size 0\"\n",
    "    assert (uix <= len(a)-1), \"index \" + str(uix) + \" is out of bound for array of size \" + str(len(a))\n",
    "\n",
    "    return a[lix], a[uix]\n",
    "\n",
    "def compute_degree_of_clustering(genes_list, batches_list, morphology,path_of_data,compartment='_branches'):\n",
    "    individual_cells = []\n",
    "    for batch in batches_list:\n",
    "        for morph in morphology:\n",
    "            load_path = f\"{path_of_data}{batch}/morph_{morph}/degree_of_clustering{compartment}.npy\"\n",
    "            if os.path.exists(load_path):\n",
    "                individual_cells.append(np.load(load_path))\n",
    "        \n",
    "    cluster = np.concatenate(individual_cells,axis=1)\n",
    "    cluster_list = [row[row != 0.0001] for row in cluster]\n",
    "    \n",
    "    gene2_degree_of_clustering = {}\n",
    "    gene2mean_degree_of_clustering = {}\n",
    "    gene2error_degree_of_clustering = {}\n",
    "    gene2confidence_interval = {}\n",
    "    degrees_of_clustering = []\n",
    "\n",
    "    for gene, degree_of_clustering in zip(genes, cluster_list):\n",
    "        degree_of_clustering = np.log(degree_of_clustering)\n",
    "        gene2_degree_of_clustering[gene] = degree_of_clustering\n",
    "        gene2mean_degree_of_clustering[gene] = np.mean(degree_of_clustering)\n",
    "    # Standard error and CI computation\n",
    "        gene2error_degree_of_clustering[gene] = sem(degree_of_clustering, factor=0)\n",
    "        if len(degree_of_clustering) > 1:\n",
    "            lower, higher = mean_confidence_interval(degree_of_clustering)\n",
    "        else:\n",
    "            lower=0\n",
    "            higher=0\n",
    "        gene2confidence_interval[gene] = [lower, higher]\n",
    "    return gene2_degree_of_clustering, gene2mean_degree_of_clustering, gene2error_degree_of_clustering, gene2confidence_interval\n",
    "\n",
    "def get_filtered_genes(median_values, error_values):\n",
    "    \"\"\"\n",
    "    Filters genes with non-zero errors from the median values dictionary.\n",
    "\n",
    "    Args:\n",
    "    - median_values: Dictionary of median values {gene: value, ...}.\n",
    "    - error_values: Dictionary of error values {gene: error, ...}.\n",
    "\n",
    "    Returns:\n",
    "    - List of tuples [(gene, median_value), ...] for genes with non-zero errors.\n",
    "    \"\"\"\n",
    "    # Filter genes with non-zero errors\n",
    "    filtered_genes = [\n",
    "        (gene, median) for gene, median in median_values.items()\n",
    "        if error_values.get(gene, 0) != 0\n",
    "    ]\n",
    "    return filtered_genes\n",
    "\n",
    "def find_genes_with_largest_deviation_and_significance(\n",
    "    filtered_young_genes, \n",
    "    filtered_old_genes, \n",
    "    median_young, \n",
    "    median_old, \n",
    "    d_of_c_young, \n",
    "    d_of_c_old, \n",
    "    significance_threshold=0.05\n",
    "):\n",
    "    \"\"\"\n",
    "    Identify genes in both filtered young and old genes, compute their deviation, \n",
    "    and filter by significant p-values.\n",
    "\n",
    "    Args:\n",
    "    - filtered_young_genes: List of tuples [(gene, median_value_y), ...] for young samples.\n",
    "    - filtered_old_genes: List of tuples [(gene, median_value_o), ...] for old samples.\n",
    "    - median_young: Dictionary of medians for young samples {gene: value, ...}.\n",
    "    - median_old: Dictionary of medians for old samples {gene: value, ...}.\n",
    "    - d_of_c_young: Dictionary of d_of_c arrays for young samples {gene: array, ...}.\n",
    "    - d_of_c_old: Dictionary of d_of_c arrays for old samples {gene: array, ...}.\n",
    "    - significance_threshold: Threshold for p-value significance.\n",
    "\n",
    "    Returns:\n",
    "    - List of tuples [(gene, deviation, p-value), ...] filtered by significant p-values.\n",
    "    \"\"\"\n",
    "    from scipy.stats import ranksums\n",
    "    import numpy as np\n",
    "\n",
    "    # Extract the gene names from the filtered lists\n",
    "    young_genes_set = {gene for gene, _ in filtered_young_genes}\n",
    "    old_genes_set = {gene for gene, _ in filtered_old_genes}\n",
    "    \n",
    "    # Find common genes in both filtered lists\n",
    "    common_genes = young_genes_set.intersection(old_genes_set)\n",
    "    \n",
    "    # Compute deviations and p-values for the common genes\n",
    "    significant_genes = []\n",
    "    for gene in common_genes:\n",
    "        young_value = median_young[gene]\n",
    "        old_value = median_old[gene]\n",
    "        \n",
    "        # Compute deviation\n",
    "        deviation = abs(young_value - old_value)\n",
    "        if np.isnan(deviation):\n",
    "            deviation = 0\n",
    "        \n",
    "        # Perform t-test\n",
    "        young_values = d_of_c_young.get(gene, [])\n",
    "        old_values = d_of_c_old.get(gene, [])\n",
    "        stat, p_value = ranksums(young_values, old_values)\n",
    "        \n",
    "        # Check significance\n",
    "        if p_value < significance_threshold:\n",
    "            significant_genes.append((gene, deviation, p_value))\n",
    "    \n",
    "    # Sort by largest deviation\n",
    "    sorted_genes = sorted(significant_genes, key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    return sorted_genes\n",
    "\n",
    "def plot_hued_median_bar(young_dict, old_dict,ylim=None,output_pdf=None):\n",
    "    # Extract genes, medians, and errors for young and old samples\n",
    "    all_genes = list(young_dict[1].keys())  # Assuming the same genes are in both dictionaries\n",
    "    young_medians = list(young_dict[1].values())\n",
    "    young_err = list(young_dict[2].values())\n",
    "    old_medians = list(old_dict[1].values())\n",
    "    old_err = list(old_dict[2].values())\n",
    "    \n",
    "    bar_width = 0.35  # Width of individual bars\n",
    "    ind = np.arange(len(all_genes))  # X locations for the groups\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))  # Adjust figure size as needed\n",
    "    \n",
    "    # Plot bars for young and old\n",
    "    ax.bar(ind - bar_width/2, young_medians, bar_width, yerr=young_err, label=\"Young\", color='skyblue')\n",
    "    ax.bar(ind + bar_width/2, old_medians, bar_width, yerr=old_err, label=\"Old\", color='salmon')\n",
    "    \n",
    "    # Customize the axes\n",
    "    ax.set_xlabel(\"Genes\", fontsize=15)\n",
    "    ax.set_ylabel(\"Mean Values\", fontsize=15)\n",
    "    ax.set_title(\"Comparison of Gene Expression Between Young and Old Samples\", fontsize=18)\n",
    "    ax.set_xticks(ind)\n",
    "    ax.set_xticklabels(all_genes, fontsize=10)\n",
    "    ax.yaxis.grid(which=\"major\", color='black', linestyle='-', linewidth=0.25)\n",
    "    plt.yticks(fontsize=12)\n",
    "    plt.xticks(fontsize=10, rotation=45)  # Optional: rotate gene labels for clarity\n",
    "    \n",
    "    # Style the ticks and spines\n",
    "    ax.tick_params(right=False, top=False, bottom=True, direction='out', length=8, width=3, colors='black')\n",
    "    ax.spines['left'].set_linewidth(3)\n",
    "    if ylim:\n",
    "        ax.set_ylim(ylim[0],ylim[1])\n",
    "        \n",
    "    if output_pdf:\n",
    "        plt.savefig(output_pdf,format='pdf')\n",
    "    \n",
    "    # Add a legend\n",
    "    ax.legend(fontsize=12)\n",
    "    \n",
    "    # Show the plot\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "700eb403-f0b6-427a-8b62-dc06b0ba5f87",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path = 'cluster_output/'\n",
    "morphologies = ['0','1','2','3','4']\n",
    "batches =['3-mo-male-1',\n",
    "          '3-mo-male-2',\n",
    "          '3-mo-male-3-rev2',\n",
    "          '3-mo-female-1-rev2',\n",
    "          '3-mo-female-2',\n",
    "          '3-mo-female-3',\n",
    "          '24-mo-male-1',\n",
    "          '24-mo-male-2',\n",
    "          '24-mo-male-4-rev2',\n",
    "          '24-mo-female-1',\n",
    "          '24-mo-female-3',\n",
    "          '24-mo-female-5']\n",
    "base_data_path = '/hpc/projects/group.quake/doug/Shapes_Spatial/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "532fb46e-2446-4085-98f7-9e25d2072bb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "transcripts = find_filtered_transcripts(base_data_path+batches[-1]+'/')\n",
    "genes = np.unique(transcripts.gene.unique().tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c640b11-bc4d-4b5c-8290-66472c71f603",
   "metadata": {},
   "outputs": [],
   "source": [
    "batches_3m = [\n",
    "          '3-mo-male-1',\n",
    "          '3-mo-male-2',\n",
    "          '3-mo-male-3-rev2',\n",
    "          '3-mo-female-1-rev2',\n",
    "          '3-mo-female-2',\n",
    "          '3-mo-female-3'\n",
    "]\n",
    "\n",
    "batches_24m = [\n",
    "          '24-mo-male-1',\n",
    "          '24-mo-male-2',\n",
    "          '24-mo-male-4-rev2',\n",
    "          '24-mo-female-1',\n",
    "          '24-mo-female-3',\n",
    "          '24-mo-female-5'\n",
    "]\n",
    "\n",
    "d_of_c_b_y, median_d_of_c_b_y, err_b_y, confidence_interval_b_y = compute_degree_of_clustering(genes, batches_3m, ['3','4'],output_path,compartment='_branches')\n",
    "d_of_c_b_o, median_d_of_c_b_o, err_b_o, confidence_interval_b_o = compute_degree_of_clustering(genes, batches_24m, ['3','4'],output_path,compartment='_branches')\n",
    "\n",
    "d_of_c_s_y, median_d_of_c_s_y, err_s_y, confidence_interval_s_y = compute_degree_of_clustering(genes, batches_3m, ['3','4'],output_path,compartment='_soma')\n",
    "d_of_c_s_o, median_d_of_c_s_o, err_s_o, confidence_interval_s_o = compute_degree_of_clustering(genes, batches_24m, ['3','4'],output_path,compartment='_soma')\n",
    "\n",
    "filtered_y_s = get_filtered_genes(median_d_of_c_s_y, err_s_y)\n",
    "filtered_o_s = get_filtered_genes(median_d_of_c_s_o, err_s_o)\n",
    "\n",
    "filtered_y_b = get_filtered_genes(median_d_of_c_b_y, err_b_y)\n",
    "filtered_o_b = get_filtered_genes(median_d_of_c_b_o, err_b_o)\n",
    "\n",
    "largest_significant_s_genes = find_genes_with_largest_deviation_and_significance(\n",
    "    filtered_y_s, \n",
    "    filtered_o_s, \n",
    "    median_d_of_c_s_y, \n",
    "    median_d_of_c_s_o, \n",
    "    d_of_c_s_y, \n",
    "    d_of_c_s_o, \n",
    "    significance_threshold=0.05\n",
    ")\n",
    "\n",
    "largest_significant_b_genes = find_genes_with_largest_deviation_and_significance(\n",
    "    filtered_y_b, \n",
    "    filtered_o_b, \n",
    "    median_d_of_c_b_y, \n",
    "    median_d_of_c_b_o, \n",
    "    d_of_c_b_y, \n",
    "    d_of_c_b_o, \n",
    "    significance_threshold=0.05\n",
    ")\n",
    "\n",
    "print(\"Significant Soma Genes with Largest Deviations:\")\n",
    "soma_genes = []\n",
    "for gene, deviation, p_value in largest_significant_s_genes:\n",
    "    print(f\"Gene: {gene}, Deviation: {deviation:.4f}, P-Value: {p_value:.4f}\")\n",
    "    soma_genes.append(gene)\n",
    "    \n",
    "print(\"Significant Branch Genes with Largest Deviations:\")\n",
    "branch_genes = []\n",
    "for gene, deviation, p_value in largest_significant_b_genes:\n",
    "    print(f\"Gene: {gene}, Deviation: {deviation:.4f}, P-Value: {p_value:.4f}\")\n",
    "    branch_genes.append(gene)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1afd86bf-2e21-40b6-96ca-f64d6fb45c5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "branch_young_dicts = [d_of_c_b_y,median_d_of_c_b_y,err_b_y,confidence_interval_b_y]\n",
    "branch_old_dicts = [d_of_c_b_o,median_d_of_c_b_o,err_b_o,confidence_interval_b_o]\n",
    "soma_young_dicts = [d_of_c_s_y,median_d_of_c_s_y,err_s_y,confidence_interval_s_y]\n",
    "soma_old_dicts = [d_of_c_s_o,median_d_of_c_s_o,err_s_o,confidence_interval_s_o]\n",
    "\n",
    "branch_young_dicts = [{key: d[key] for key in branch_genes} for d in branch_young_dicts]\n",
    "branch_old_dicts = [{key: d[key] for key in branch_genes} for d in branch_old_dicts]\n",
    "soma_young_dicts = [{key: d[key] for key in soma_genes} for d in soma_young_dicts]\n",
    "soma_old_dicts = [{key: d[key] for key in soma_genes} for d in soma_old_dicts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a0f4bb6-1d60-4dc3-961b-984e73862e03",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_hued_median_bar(soma_young_dicts,soma_old_dicts,output_pdf='Figures/Figure_coloc/soma_cluster.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "382f50c0-354a-4e49-837b-65c6a24d9d72",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_hued_median_bar(branch_young_dicts,branch_old_dicts,ylim=[4,6],output_pdf='Figures/Figure_coloc/branch_cluster.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30dca7da-2468-4f08-868a-040cb1e904a7",
   "metadata": {},
   "source": [
    "# Colocalization Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39a098ce-b4a8-4735-9589-25d937658e3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_zero_pairs(contact_mtx):\n",
    "    n_0 = 0\n",
    "    for i in range(contact_mtx.shape[0]):\n",
    "        for j in range(i, contact_mtx.shape[0]):\n",
    "            if contact_mtx[i, j] == 0:\n",
    "                n_0 += 1\n",
    "    return n_0\n",
    "\n",
    "def adjust_p_value_matrix_by_BH(p_val_mtx):\n",
    "    '''Adjust the p-values in a matrix by the Benjamini/Hochberg method.\n",
    "    The matrix should be symmetric.\n",
    "    '''\n",
    "    p_val_sequential = []\n",
    "    N = p_val_mtx.shape[0]\n",
    "    \n",
    "    for i in range(N):\n",
    "        for j in range(i, N):\n",
    "            p_val_sequential.append(p_val_mtx[i, j])\n",
    "\n",
    "    p_val_sequential_bh = statsmodels.stats.multitest.multipletests(p_val_sequential, method='fdr_bh')[1]\n",
    "    \n",
    "    adjusted_p_val_mtx = np.zeros((N, N))\n",
    "    \n",
    "    counter = 0\n",
    "    for i in range(N):\n",
    "        for j in range(i, N):\n",
    "            adjusted_p_val_mtx[i, j] = p_val_sequential_bh[counter]\n",
    "            adjusted_p_val_mtx[j, i] = p_val_sequential_bh[counter]\n",
    "            counter += 1\n",
    "            \n",
    "    return adjusted_p_val_mtx\n",
    "\n",
    "def get_data_frame_from_metrices(cell_types, mtx_dict):\n",
    "    N = len(cell_types)\n",
    "    \n",
    "    serials_dict = {'cell_type1':[], 'cell_type2':[]}\n",
    "    for k in mtx_dict.keys():\n",
    "        serials_dict[k] = []\n",
    "        \n",
    "    for i in range(N):\n",
    "        for j in range(i, N):\n",
    "            serials_dict['cell_type1'].append(cell_types[i])\n",
    "            serials_dict['cell_type2'].append(cell_types[j])\n",
    "            for k in mtx_dict.keys():\n",
    "                serials_dict[k].append(mtx_dict[k][i, j])\n",
    "                \n",
    "    return pd.DataFrame(serials_dict)\n",
    "    \n",
    "\n",
    "def sort_cell_type_contact_p_values(p_val_mtx, cell_types):\n",
    "    '''Return a list of (cell_type1, cell_type2, p_value) sorted by p_values.'''\n",
    "    p_val_list = []\n",
    "    N = p_val_mtx.shape[0]\n",
    "    for i in range(N):\n",
    "        for j in range(i, N):\n",
    "            p_val_list.append((cell_types[i], cell_types[j], p_val_mtx[i, j]))\n",
    "    return sorted(p_val_list, key=lambda x:x[2])\n",
    "\n",
    "import scipy.cluster\n",
    "#from scattermap import scattermap\n",
    "\n",
    "def get_optimal_order_of_mtx(X):\n",
    "    Z = scipy.cluster.hierarchy.ward(X)\n",
    "    return scipy.cluster.hierarchy.leaves_list(\n",
    "        scipy.cluster.hierarchy.optimal_leaf_ordering(Z, X))\n",
    "\n",
    "def get_ordered_tick_labels(tick_labels):\n",
    "    tick_labels_with_class = [s.split(' ')[-1] + ' ' + s for s in tick_labels]\n",
    "    return np.argsort(tick_labels_with_class)\n",
    "\n",
    "def filter_pval_mtx(pval_mtx, tick_labels, allowed_pairs):\n",
    "    pval_mtx_filtered = pval_mtx.copy()\n",
    "    \n",
    "    for i in range(pval_mtx.shape[0]):\n",
    "        ct1 = tick_labels[i]\n",
    "        for j in range(pval_mtx.shape[1]):\n",
    "            ct2 = tick_labels[j]\n",
    "            \n",
    "            if ((ct1, ct2) in allowed_pairs) or ((ct2, ct1) in allowed_pairs):\n",
    "                continue\n",
    "            else:\n",
    "                pval_mtx_filtered[i, j] = 1\n",
    "            \n",
    "    return pval_mtx_filtered\n",
    "\n",
    "def make_dotplot(pval_mtx, fold_change_mtx, tick_labels, title='', allowed_pairs=None):\n",
    "\n",
    "    #optimal_order = get_optimal_order_of_mtx(pval_mtx)\n",
    "    optimal_order = get_ordered_tick_labels(tick_labels)\n",
    "    \n",
    "    pval_mtx = pval_mtx[optimal_order][:, optimal_order]\n",
    "    fold_change_mtx = fold_change_mtx[optimal_order][:, optimal_order]\n",
    "    tick_labels = tick_labels[optimal_order]\n",
    "    \n",
    "    \n",
    "def find_filtered_transcripts(experiment_path):\n",
    "    region_types = ['region_0', 'region_1']\n",
    "    for region in region_types:\n",
    "        file_path = f'{experiment_path}baysor/detected_transcripts.csv'\n",
    "        if os.path.exists(file_path):\n",
    "            return pd.read_csv(file_path,index_col=0)\n",
    "    return None\n",
    "\n",
    "def permutation_analysis(base_path, batch_list, morphology_list, output_p,compartment='soma'):\n",
    "    full_df = []\n",
    "    # so I want to iterate through each geometry class\n",
    "    for morph in morphology_list:\n",
    "        # then through every batch\n",
    "        for batch in batch_list:\n",
    "            # load the gene names\n",
    "            transcripts = find_filtered_transcripts(base_path+batch+'/')\n",
    "            genes = np.unique(transcripts.gene.unique().tolist())\n",
    "    \n",
    "            gene_coloc_counts = np.load(f'{output_p}{batch}/morph_{morph}/{compartment}_no_permutation.npy')\n",
    "\n",
    "            local_null_means = np.load(f'{output_p}{batch}/morph_{morph}/{compartment}_full_permutation_mean.npy')\n",
    "            local_null_stds = np.load(f'{output_p}{batch}/morph_{morph}/{compartment}_full_permutation_std.npy')\n",
    "\n",
    "            # Require all stds to be larger or equal to the minimal observable std value\n",
    "            local_null_stds = np.maximum(local_null_stds, np.sqrt(1 / 1000))\n",
    "    \n",
    "            local_z_scores = (gene_coloc_counts - local_null_means) / local_null_stds\n",
    "            local_p_values = scipy.stats.norm.sf(local_z_scores)\n",
    "            adjusted_local_p_values = adjust_p_value_matrix_by_BH(local_p_values)\n",
    "    \n",
    "            fold_changes = gene_coloc_counts / (local_null_means + 1e-4)\n",
    "        \n",
    "            # Gather all results into a data frame\n",
    "            contact_result_df = get_data_frame_from_metrices(genes, \n",
    "                                             {'pval-adjusted': adjusted_local_p_values,\n",
    "                                              'pval': local_p_values,\n",
    "                                              'z_score': local_z_scores,\n",
    "                                              'contact_count': gene_coloc_counts,\n",
    "                                              'permutation_mean': local_null_means,\n",
    "                                              'permutation_std': local_null_stds,\n",
    "                                            }).sort_values('z_score', ascending=False)\n",
    "\n",
    "            \n",
    "            contact_result_df.to_csv(f'{output_p}{batch}/morph_{morph}/{compartment}_close_contacts.csv')\n",
    "    \n",
    "            full_df.append(contact_result_df)\n",
    "        \n",
    "    return full_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "257d4b5e-4aa3-470b-b800-1f2200a0d305",
   "metadata": {},
   "outputs": [],
   "source": [
    "# name all of the necessary variables\n",
    "\n",
    "output_path = 'permutation_coloc/'\n",
    "morphologies = ['0','1','2','3','4']\n",
    "batches =['3-mo-male-1',\n",
    "          '3-mo-male-2',\n",
    "          '3-mo-male-3-rev2',\n",
    "          '3-mo-female-1-rev2',\n",
    "          '3-mo-female-2',\n",
    "          '3-mo-female-3',\n",
    "          '24-mo-male-1',\n",
    "          '24-mo-male-2',\n",
    "          '24-mo-male-4-rev2',\n",
    "          '24-mo-female-1',\n",
    "          '24-mo-female-3',\n",
    "          '24-mo-female-5']\n",
    "\n",
    "# replace with where the data is kept\n",
    "base_data_path = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c21ce7eb-44c7-429b-a6ab-f1db23a77720",
   "metadata": {},
   "outputs": [],
   "source": [
    "soma_df = permutation_analysis(base_data_path, batches, morphologies, output_path,compartment='soma')\n",
    "branches_df = permutation_analysis(base_data_path, batches, morphologies, output_path,compartment='branches')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c96ac6f-d91b-417d-82a3-0947ede60896",
   "metadata": {},
   "source": [
    "# Trimming the dataframes and looking at specific morphologies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25bac60a-6aa6-4274-8217-9db676743e3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_disconnected_pairs(df, node1_col, node2_col):\n",
    "    \"\"\"\n",
    "    Remove disconnected components with only two nodes from the graph created by the DataFrame.\n",
    "    Args:\n",
    "        df (pd.DataFrame): Input DataFrame containing at least two columns representing edges in a graph.\n",
    "        node1_col (str): Name of the first column representing nodes (e.g., 'cell_type1').\n",
    "        node2_col (str): Name of the second column representing nodes (e.g., 'cell_type2').\n",
    "    Returns:\n",
    "        pd.DataFrame: Filtered DataFrame containing only edges from valid components.\n",
    "    \"\"\"\n",
    "    # Create a graph from the DataFrame\n",
    "    G = nx.Graph()\n",
    "    G.add_edges_from(zip(df[node1_col], df[node2_col]))\n",
    "\n",
    "    # Identify connected components\n",
    "    components = list(nx.connected_components(G))\n",
    "\n",
    "    # Keep only components with more than two nodes\n",
    "    valid_components = [comp for comp in components if len(comp) > 2]\n",
    "\n",
    "    # Flatten the valid components into a set of nodes\n",
    "    valid_nodes = set(node for comp in valid_components for node in comp)\n",
    "\n",
    "    # Filter the DataFrame to retain only rows where both nodes are in valid components\n",
    "    filtered_df = df[\n",
    "        (df[node1_col].isin(valid_nodes)) &\n",
    "        (df[node2_col].isin(valid_nodes))\n",
    "    ]\n",
    "    return filtered_df\n",
    "\n",
    "def extract_coloc_tables(morphologies, batches, contact_thresh=4, pval_thresh=0.05):\n",
    "    \"\"\"\n",
    "    Extract and filter colocalization tables for soma and branches, \n",
    "    including filtering out disconnected graph components with only two nodes.\n",
    "\n",
    "    Args:\n",
    "        morphologies (list): List of morphology types.\n",
    "        batches (list): List of batch identifiers.\n",
    "        contact_thresh (int): Minimum contact count threshold for filtering.\n",
    "        pval_thresh (float): Maximum adjusted p-value threshold for filtering.\n",
    "\n",
    "    Returns:\n",
    "        tuple: Filtered soma and branches DataFrames.\n",
    "    \"\"\"\n",
    "    soma_df = []\n",
    "    branches_df = []\n",
    "\n",
    "    # Iterate over morphologies and batches to load data\n",
    "    for morph in morphologies:\n",
    "        for batch in batches:\n",
    "            soma_df.append(pd.read_csv(f'{output_path}{batch}/morph_{morph}/soma_close_contacts.csv', index_col=0))\n",
    "            branches_df.append(pd.read_csv(f'{output_path}{batch}/morph_{morph}/branches_close_contacts.csv', index_col=0))\n",
    "\n",
    "    # Combine all data\n",
    "    soma_full = pd.concat(soma_df)\n",
    "    branches_full = pd.concat(branches_df)\n",
    "\n",
    "    # Apply initial filtering based on thresholds\n",
    "    soma_full = soma_full[\n",
    "        (soma_full.contact_count > contact_thresh) &\n",
    "        (soma_full['pval-adjusted'] < pval_thresh) &\n",
    "        (soma_full.cell_type1 != soma_full.cell_type2)\n",
    "    ]\n",
    "\n",
    "    branches_full = branches_full[\n",
    "        (branches_full.contact_count > contact_thresh) &\n",
    "        (branches_full['pval-adjusted'] < pval_thresh) &\n",
    "        (branches_full.cell_type1 != branches_full.cell_type2)\n",
    "    ]\n",
    "\n",
    "    # Group by cell_type1 and cell_type2, averaging numerical values\n",
    "    soma_full = soma_full.groupby(['cell_type1', 'cell_type2']).mean().reset_index()\n",
    "    branches_full = branches_full.groupby(['cell_type1', 'cell_type2']).mean().reset_index()\n",
    "\n",
    "    # Apply graph-based filtering to remove disconnected pairs\n",
    "    soma_full = filter_disconnected_pairs(soma_full, 'cell_type1', 'cell_type2')\n",
    "    branches_full = filter_disconnected_pairs(branches_full, 'cell_type1', 'cell_type2')\n",
    "\n",
    "    return soma_full, branches_full\n",
    "\n",
    "def save_dataframes_to_excel(dataframes, sheet_names, output_path):\n",
    "    \"\"\"\n",
    "    Save multiple DataFrames to a single Excel file, with each DataFrame on a separate sheet.\n",
    "\n",
    "    Args:\n",
    "        dataframes (list): List of pandas DataFrames to save.\n",
    "        sheet_names (list): List of sheet names corresponding to each DataFrame.\n",
    "        output_path (str): Path to save the Excel file (e.g., 'output.xlsx').\n",
    "    \"\"\"\n",
    "    with pd.ExcelWriter(output_path, engine='openpyxl') as writer:\n",
    "        for df, sheet_name in zip(dataframes, sheet_names):\n",
    "            df.to_excel(writer, sheet_name=sheet_name, index=False)\n",
    "    print(f\"DataFrames saved to {output_path}\")\n",
    "\n",
    "def perform_go_enrichment(clusters_dict, organism='Mouse'):\n",
    "    \"\"\"\n",
    "    Perform GO enrichment analysis for each cluster and save results.\n",
    "\n",
    "    Args:\n",
    "        clusters_dict (dict): Dictionary where keys are cluster IDs and values are lists of genes.\n",
    "        output_dir (str): Directory to save GO enrichment results.\n",
    "        organism (str): Organism for enrichment analysis ('Human' or 'Mouse').\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary where keys are cluster IDs and values are DataFrames of enrichment results.\n",
    "    \"\"\"\n",
    "    go_results = {}\n",
    "    \n",
    "    for cluster_id, genes in clusters_dict.items():\n",
    "        # Perform GO enrichment analysis\n",
    "        enrichment_results = gp.enrichr(\n",
    "            gene_list=genes,  # List of genes in the cluster\n",
    "            gene_sets='GO_Biological_Process_2023',  # Gene Ontology Biological Process\n",
    "            organism=organism\n",
    "        )\n",
    "        \n",
    "        # Store results in dictionary\n",
    "        go_results[cluster_id] = enrichment_results.results\n",
    "    \n",
    "    return go_results\n",
    "\n",
    "def plot_contact_network_with_gmm(df, title, threshold=0.05, n_components=3, save_path=None):\n",
    "    \"\"\"\n",
    "    Plot a gene-gene contact network and identify clusters using GMM after handling connected components.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): Dataframe with gene-gene interaction data.\n",
    "        title (str): Title of the plot.\n",
    "        threshold (float): p-value threshold to filter significant interactions.\n",
    "        n_components (int): Number of GMM components (clusters) for each connected component.\n",
    "        save_path (str): Path to save the figure (optional).\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary where keys are cluster IDs and values are lists of nodes in each cluster.\n",
    "    \"\"\"\n",
    "    # Step 1: Filter data based on p-value threshold\n",
    "    df_filtered = df[df['pval-adjusted'] < threshold]\n",
    "\n",
    "    # Step 2: Create a network graph\n",
    "    G = nx.Graph()\n",
    "    for _, row in df_filtered.iterrows():\n",
    "        G.add_edge(row['cell_type1'], row['cell_type2'], weight=row['z_score'])\n",
    "\n",
    "    # Step 3: Identify connected components\n",
    "    connected_components = list(nx.connected_components(G))\n",
    "    clusters = {}\n",
    "    global_cluster_id = 0  # To ensure unique cluster IDs\n",
    "\n",
    "    # Step 4: Process each connected component\n",
    "    for component in connected_components:\n",
    "        subgraph = G.subgraph(component)\n",
    "        num_nodes = len(subgraph.nodes())\n",
    "        \n",
    "        # If the component has fewer than 10 nodes, treat it as a single cluster\n",
    "        if num_nodes < 10:\n",
    "            for node in subgraph.nodes():\n",
    "                clusters[node] = global_cluster_id  # Assign all nodes to one cluster\n",
    "            global_cluster_id += 1  # Increment the cluster ID counter\n",
    "        else:\n",
    "            # Generate embeddings using Node2Vec\n",
    "            node2vec = Node2Vec(subgraph, dimensions=16, walk_length=30, num_walks=200, workers=4)\n",
    "            model = node2vec.fit(window=10, min_count=1, batch_words=4)\n",
    "            embeddings = np.array([model.wv[str(node)] for node in subgraph.nodes()])\n",
    "            \n",
    "            # Fit Gaussian Mixture Model to embeddings\n",
    "            gmm = GaussianMixture(n_components=min(n_components, len(embeddings)), random_state=42)\n",
    "            cluster_labels = gmm.fit_predict(embeddings)\n",
    "            \n",
    "            # Assign cluster IDs globally\n",
    "            for node, local_cluster_id in zip(subgraph.nodes(), cluster_labels):\n",
    "                clusters[node] = global_cluster_id + local_cluster_id\n",
    "            \n",
    "            # Update global cluster ID counter\n",
    "            global_cluster_id += max(cluster_labels) + 1\n",
    "\n",
    "    # Step 5: Visualization\n",
    "    pos = nx.spring_layout(G, seed=42)  # Layout for consistent visualization\n",
    "    cluster_colors = [clusters[node] for node in G.nodes()]  # Node colors based on clusters\n",
    "\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    nx.draw_networkx_nodes(G, pos, node_size=400, node_color=cluster_colors, cmap=plt.cm.Set3, alpha=0.8)\n",
    "    nx.draw_networkx_edges(G, pos, width=1.0, edge_color='gray', alpha=0.5)\n",
    "    nx.draw_networkx_labels(G, pos, font_size=8, font_weight='bold')\n",
    "\n",
    "    # Add a legend for clusters\n",
    "    unique_clusters = set(cluster_colors)\n",
    "    legend_elements = [\n",
    "        Line2D([0], [0], marker='o', color='w', label=f'Cluster {cid}',\n",
    "               markerfacecolor=plt.cm.Set3(cid / max(unique_clusters)), markersize=10)\n",
    "        for cid in unique_clusters\n",
    "    ]\n",
    "    plt.legend(handles=legend_elements, title='Clusters', loc='upper right')\n",
    "\n",
    "    # Add title\n",
    "    plt.title(title, fontsize=16)\n",
    "\n",
    "    if save_path:\n",
    "        plt.savefig(f'coloc_figs/{save_path}', format='pdf')\n",
    "\n",
    "    plt.show()\n",
    "    \n",
    "    # Step 6: Invert the clusters dictionary\n",
    "    inverted_clusters = {}\n",
    "    for gene, cluster_id in clusters.items():\n",
    "        if cluster_id not in inverted_clusters:\n",
    "            inverted_clusters[cluster_id] = []  # Initialize list if not already present\n",
    "        inverted_clusters[cluster_id].append(gene)\n",
    "\n",
    "    return inverted_clusters\n",
    "\n",
    "def plot_go_enrichment(go_results, top_n=5, save_path=None, global_max_score=None, figure_width=10):\n",
    "    \"\"\"\n",
    "    Plot a horizontal bar graph of the top GO terms for each cluster, ensuring consistent graph sizes.\n",
    "    \n",
    "    Args:\n",
    "        go_results (dict): Dictionary where keys are cluster IDs and values are DataFrames of GO enrichment results.\n",
    "        top_n (int): Number of top GO terms to display per cluster.\n",
    "        save_path (str): Path to save the figure as a PDF (optional).\n",
    "        global_max_score (float): Global maximum score for x-axis synchronization (optional).\n",
    "        figure_width (float): Fixed width of the graph area (excluding labels).\n",
    "    \"\"\"\n",
    "    # Prepare data for plotting\n",
    "    plot_data = []\n",
    "    for cluster_id, df in go_results.items():\n",
    "        # Select top N GO terms based on adjusted p-value\n",
    "        top_terms = df.sort_values('Adjusted P-value').head(top_n)\n",
    "        for _, row in top_terms.iterrows():\n",
    "            plot_data.append({\n",
    "                'Cluster': cluster_id,\n",
    "                'GO Term': row['Term'],\n",
    "                '-log10(P-value)': -np.log10(row['Adjusted P-value']),\n",
    "                'Combined Score': row['Combined Score']\n",
    "            })\n",
    "    \n",
    "    # Convert to DataFrame\n",
    "    plot_df = pd.DataFrame(plot_data)\n",
    "    \n",
    "    # Determine the x-axis limit\n",
    "    max_score = global_max_score or plot_df['Combined Score'].max()\n",
    "\n",
    "    # Estimate space needed for y-axis labels\n",
    "    max_label_length = plot_df['GO Term'].str.len().max()\n",
    "    label_width = max_label_length * 0.1  # Estimate label width in inches (adjust scaling factor as needed)\n",
    "    total_width = figure_width + label_width\n",
    "\n",
    "    # Plot horizontal bar graph\n",
    "    plt.figure(figsize=(total_width, len(plot_df) * 0.4))\n",
    "    sns.barplot(\n",
    "        data=plot_df,\n",
    "        y='GO Term',\n",
    "        x='Combined Score',\n",
    "        hue='Cluster',\n",
    "        dodge=False,  # Avoid overlapping bars\n",
    "        palette='Set2'\n",
    "    )\n",
    "    plt.xlabel('Combined Score', fontsize=12)\n",
    "    plt.ylabel('GO Term', fontsize=12)\n",
    "    plt.xlim([0, max_score])  # Use the global max score for consistent x-axes\n",
    "    plt.title('GO Enrichment Analysis by Cluster', fontsize=14)\n",
    "    plt.legend(title='Cluster', loc='best')\n",
    "    plt.tight_layout()\n",
    "    if save_path:\n",
    "        plt.savefig(f'coloc_figs/{save_path}', format='pdf')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "852ccec1-9afd-4747-bd35-fcfa00c9e253",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the batches (split by age) and most complex morphology\n",
    "morphologies = ['3','4']\n",
    "\n",
    "batches_3 =['3-mo-male-1',\n",
    "          '3-mo-male-2',\n",
    "          '3-mo-male-3-rev2',\n",
    "          '3-mo-female-1-rev2',\n",
    "          '3-mo-female-2',\n",
    "          '3-mo-female-3']\n",
    "\n",
    "batches_24 =['24-mo-male-1',\n",
    "          '24-mo-male-2',\n",
    "          '24-mo-male-4-rev2',\n",
    "          '24-mo-female-1',\n",
    "          '24-mo-female-3',\n",
    "          '24-mo-female-5']\n",
    "\n",
    "young_4_soma, young_4_branches = extract_coloc_tables(morphologies, batches_3)\n",
    "old_4_soma, old_4_branches = extract_coloc_tables(morphologies, batches_24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3d40d67-b600-4744-88f9-e3fa3f5dddf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframes = [young_4_soma, young_4_branches, old_4_soma, old_4_branches]\n",
    "sheet_names = ['Young_Soma', 'Young_Processes', 'Old_Soma', 'Old_Processes']\n",
    "\n",
    "output_file = 'coloc_figs/gene_colocalization_networks.xlsx'\n",
    "save_dataframes_to_excel(dataframes, sheet_names, output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49a5ae55-fdf0-4aa3-b8ff-a612d3c35006",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the graphs of the colocalization networks, clusters set to 3 by default, can manually tune\n",
    "young_soma_gene_clusters = plot_contact_network_with_gmm(young_soma_com, '3 month soma',n_components=2, threshold=0.05,save_path='3_month_soma.pdf')\n",
    "young_branches_gene_clusters = plot_contact_network_with_gmm(young_branches_com, '3 month branches', threshold=0.05,n_components=2,save_path='3_month_branches.pdf')\n",
    "old_soma_gene_clusters = plot_contact_network_with_gmm(old_soma_com, '24 month soma', threshold=0.05,n_components=1,save_path='24_month_soma.pdf')\n",
    "old_branches_gene_clusters = plot_contact_network_with_gmm(old_branches_com, '24 month branches', threshold=0.05,n_components=2,save_path='24_month_branches.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1515d40-6239-4bc5-bc7d-9679b0d83e6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3 month soma gene ontology\n",
    "results = perform_go_enrichment(young_soma_gene_clusters)\n",
    "plot_go_enrichment(results, top_n=5,save_path='3_month_soma_GO.pdf',global_max_score=17500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d1ce375-23e6-49b5-845c-bf2dc847d321",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 24 month soma gene ontology\n",
    "results = perform_go_enrichment(old_soma_gene_clusters)\n",
    "plot_go_enrichment(results, top_n=5,save_path='24_month_soma_GO.pdf',global_max_score=17500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7801591-af6a-491f-adf9-c0df79ea2fb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3 month branches gene ontology\n",
    "results = perform_go_enrichment(young_branches_gene_clusters)\n",
    "plot_go_enrichment(results, top_n=3,save_path='3_month_branches_GO.pdf',global_max_score=26800)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49790c3f-cb26-41b7-a47e-5410bddc4b7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 24 month branches gene ontology\n",
    "results = perform_go_enrichment(old_branches_gene_clusters)\n",
    "plot_go_enrichment(results, top_n=3,save_path='24_month_branches_GO.pdf',global_max_score=26800)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Vizgen_2",
   "language": "python",
   "name": "vizgen_2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
